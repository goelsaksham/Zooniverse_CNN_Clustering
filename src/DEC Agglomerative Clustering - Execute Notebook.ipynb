{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEC Agglomerative Clustering\n",
    "\n",
    "This notebook focuses on using Agglomerative Clustering instead of other types of clustering algorithms. The notebook will be extended to other algorithms like: Spectral Clustering, DBSCAN, refer to the webpage: https://scikit-learn.org/stable/modules/clustering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required Libaries\n",
    "\n",
    "Importing the required libraries and modules so that they can be used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_validate_lengths'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a2ecf26f9435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Using scikit-image  resize function for resizing the image from original size to 224 X 224\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# Train Test split from sklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/skimage/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0m_raise_build_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/skimage/util/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mapply_parallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marraycrop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_regular_grid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregular_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregular_seeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munique_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/skimage/util/arraycrop.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marraypad\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_validate_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_validate_lengths'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# loading the requirements for the Xception model\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import absolute_import, decode_predictions, preprocess_input\n",
    "from keras.layers import Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# Loading the DEC module cloned from github\n",
    "from DEC.model import *\n",
    "from DEC.metrics import *\n",
    "from xception_dec_datagenerator import XceptionDataGenerator\n",
    "# Importing the utilities\n",
    "from utils.file_utils import *\n",
    "from PIL import Image\n",
    "# For visualization of images and for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Using scikit-image  resize function for resizing the image from original size to 224 X 224\n",
    "# from skimage.transform import resize\n",
    "# Train Test split from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# Copy function\n",
    "from shutil import copy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xception Model\n",
    "\n",
    "First we load the Xception model into the computer memory using the Keras library. Because we are focusing on extracting features from the model we do not include the topmost layer. However we do use the imagenet weights for the model. Also because we want a 1-D vector form of the features we do use the pooling layer at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 150, 150, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 74, 74, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 74, 74, 32)   128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 74, 74, 32)   0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 72, 72, 64)   18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 72, 72, 64)   256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 72, 72, 64)   0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 72, 72, 128)  8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 72, 72, 128)  512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 72, 72, 128)  0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 72, 72, 128)  17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 72, 72, 128)  512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 36, 36, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 36, 36, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 36, 36, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 36, 36, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 36, 36, 128)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 36, 36, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 36, 36, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 36, 36, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 36, 36, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 36, 36, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 18, 18, 256)  32768       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 18, 18, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 18, 18, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 18, 18, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 18, 18, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 18, 18, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 18, 18, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 18, 18, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 18, 18, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 18, 18, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 9, 9, 728)    186368      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 9, 9, 728)    0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 9, 9, 728)    2912        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 9, 9, 728)    0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 9, 9, 728)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 9, 9, 728)    536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 9, 9, 728)    0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 9, 9, 728)    0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 9, 9, 728)    536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 9, 9, 728)    2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 9, 9, 728)    0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 9, 9, 728)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 9, 9, 728)    536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 9, 9, 728)    0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 9, 9, 728)    0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 9, 9, 728)    536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 9, 9, 728)    2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 9, 9, 728)    0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 9, 9, 728)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 9, 9, 728)    536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 9, 9, 728)    0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 9, 9, 728)    0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 9, 9, 728)    536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 9, 9, 728)    2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 9, 9, 728)    0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 9, 9, 728)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 9, 9, 728)    536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 9, 9, 728)    0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 9, 9, 728)    0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 9, 9, 728)    536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 9, 9, 728)    2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 9, 9, 728)    0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 9, 9, 728)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 9, 9, 728)    536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 9, 9, 728)    0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 9, 9, 728)    0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 9, 9, 728)    536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 9, 9, 728)    2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 9, 9, 728)    0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 9, 9, 728)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 9, 9, 728)    536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 9, 9, 728)    2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 9, 9, 728)    0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 9, 9, 728)    536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 9, 9, 728)    2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 9, 9, 728)    0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 9, 9, 728)    536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 9, 9, 728)    2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 9, 9, 728)    0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 9, 9, 728)    0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 9, 9, 728)    536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 9, 9, 728)    2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 9, 9, 728)    0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 9, 9, 728)    536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 9, 9, 728)    2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 9, 9, 728)    0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 9, 9, 728)    536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 9, 9, 728)    2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 9, 9, 728)    0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 9, 9, 728)    0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 9, 9, 728)    536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 9, 9, 728)    2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 9, 9, 728)    0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 9, 9, 728)    536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 9, 9, 728)    2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 9, 9, 728)    0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 9, 9, 728)    536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 9, 9, 728)    2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 9, 9, 728)    0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 9, 9, 728)    0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 9, 9, 728)    536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 9, 9, 728)    2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 9, 9, 728)    0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 9, 9, 1024)   752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 9, 9, 1024)   4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 5, 5, 1024)   745472      add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 5, 5, 1024)   0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 5, 5, 1024)   4096        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 5, 5, 1024)   0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 5, 5, 1536)   1582080     add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 5, 5, 1536)   6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 5, 5, 1536)   0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 5, 5, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 5, 5, 2048)   8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 5, 5, 2048)   0           block14_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           block14_sepconv2_act[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 20,861,480\n",
      "Trainable params: 20,806,952\n",
      "Non-trainable params: 54,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor_shape = (150, 150, 3)\n",
    "base_xception_model = Xception(weights = 'imagenet', input_shape = input_tensor_shape, include_top = False, pooling='avg')\n",
    "base_xception_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input shape of our base xception model is: 150 X 150 X 3. That is a 3 channel square image with side 150 pixels.\n",
    "The output shape of the base xception model is: 2048 X 1. It is a 1-D vector representing the features learned by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generators\n",
    "\n",
    "Defining the keras data generators to iterate through all the images and then essentially help in extracting the features from the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_extension = '.jpg'\n",
    "training_directory_path = f'../data/xception_clustering/training/'\n",
    "testing_directory_path = f'../data/xception_clustering/testing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of already present Training Elliptical Galaxies: 25868\n",
      "Number of already present Training Spiral Galaxies: 34105\n",
      "Number of already present Testing Elliptical Galaxies: 0\n",
      "Number of already present Testing Spiral Galaxies: 0\n"
     ]
    }
   ],
   "source": [
    "# Getting the files already in the training and testing folders respectively\n",
    "spiral_training_directory_path = construct_path(training_directory_path, 'spiral')\n",
    "elliptical_training_directory_path = construct_path(training_directory_path, 'elliptical')\n",
    "spiral_testing_directory_path = construct_path(testing_directory_path, 'spiral')\n",
    "elliptical_testing_directory_path = construct_path(testing_directory_path, 'elliptical')\n",
    "elliptical_training_files = get_file_nms(elliptical_training_directory_path, image_extension)\n",
    "spiral_training_files = get_file_nms(spiral_training_directory_path, image_extension)\n",
    "elliptical_testing_files = get_file_nms(elliptical_testing_directory_path, image_extension)\n",
    "spiral_testing_files = get_file_nms(spiral_testing_directory_path, image_extension)\n",
    "# Finding the number of images for each type of galaxy after finding the common images and list\n",
    "print(f'Number of already present Training Elliptical Galaxies: {len(elliptical_training_files)}')\n",
    "print(f'Number of already present Training Spiral Galaxies: {len(spiral_training_files)}')\n",
    "print(f'Number of already present Testing Elliptical Galaxies: {len(elliptical_testing_files)}')\n",
    "print(f'Number of already present Testing Spiral Galaxies: {len(spiral_testing_files)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization and Cropping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_difference(orig_size, target_size):\n",
    "    orig_size, target_size = list(orig_size), list(target_size)\n",
    "    ret_ls = []\n",
    "    for o, t in zip(orig_size, target_size):\n",
    "        ret_ls.append(o - t)\n",
    "    return ret_ls\n",
    "\n",
    "def crop_image(image, orig_size, target_size):\n",
    "    crop_sizes = get_difference(orig_size, target_size)\n",
    "    height_dif, width_dif = crop_sizes[0] // 2, crop_sizes[1] // 2\n",
    "    return image[height_dif:(height_dif + target_size[0]), width_dif:(width_dif + target_size[1]), :]\n",
    "\n",
    "def range_scaling(image, out_feature_range=(-1, 1)):\n",
    "    old_min, old_max = 0., 255.\n",
    "    new_min, new_max = -1., 1.\n",
    "    return ((image - old_min)/(old_max - old_min))*(new_max - new_min) + new_min\n",
    "\n",
    "def image_preprocessing_function(image, crop=True, range_scale=True):\n",
    "    \"\"\"\n",
    "    image is a 3-D image tensor (numpy array).\n",
    "    \"\"\"\n",
    "    target_image_size = input_tensor_shape\n",
    "    if crop:\n",
    "        cropped_image = crop_image(image, image.shape, target_image_size)\n",
    "    else:\n",
    "        cropped_image = image\n",
    "        \n",
    "    if range_scale:\n",
    "        final_image = range_scaling(cropped_image)\n",
    "    else:\n",
    "        final_image = cropped_image\n",
    "    return final_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 59973 images belonging to 2 classes.\n",
      "Found 0 images belonging to 2 classes.\n",
      "CPU times: user 3.27 s, sys: 528 ms, total: 3.8 s\n",
      "Wall time: 2.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generator_batch_size = 64\n",
    "# Current generator uses -1 to 1\n",
    "image_generator = ImageDataGenerator(preprocessing_function=image_preprocessing_function)\n",
    "training_generator = image_generator.flow_from_directory(training_directory_path, target_size = input_tensor_shape[:2], \n",
    "                                                         class_mode='binary', batch_size=generator_batch_size)\n",
    "testing_generator = image_generator.flow_from_directory(testing_directory_path, target_size = input_tensor_shape[:2], \n",
    "                                                         class_mode='binary', batch_size=generator_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images processed: 6400\n",
      "Number of Images processed: 12800\n",
      "Number of Images processed: 19200\n",
      "Number of Images processed: 25600\n",
      "Number of Images processed: 32000\n",
      "Number of Images processed: 38400\n",
      "Number of Images processed: 44800\n",
      "Number of Images processed: 51200\n",
      "Number of Images processed: 57600\n",
      "Shape of the training features (59968, 2048)\n",
      "CPU times: user 6min 31s, sys: 1min 31s, total: 8min 2s\n",
      "Wall time: 7min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_train_examples = (len(training_generator.filenames)//generator_batch_size) * generator_batch_size\n",
    "train_features = np.zeros((n_train_examples, 2048))\n",
    "train_labels = np.zeros(n_train_examples, dtype=int)\n",
    "i = 0\n",
    "for inputs_batch, labels_batch in training_generator:\n",
    "    features_batch = base_xception_model.predict(inputs_batch)\n",
    "    train_features[i * generator_batch_size : (i + 1) * generator_batch_size] = features_batch\n",
    "    train_labels[i * generator_batch_size : (i + 1) * generator_batch_size] = labels_batch\n",
    "    i += 1\n",
    "    if i % 100 == 0 and i:\n",
    "        print('Number of Images processed:', i * generator_batch_size)\n",
    "    if i * generator_batch_size >= n_train_examples:\n",
    "        break\n",
    "\n",
    "print('Shape of the training features', train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elliptical': 0, 'spiral': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Link Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEC Xception Training Regime\n",
    "\n",
    "This part of the notebook defines the generator for the training regime of the DEC model over the features extracted from the Xception architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 500)               1024500   \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "encoder_2 (Dense)            (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "encoder_3 (Dense)            (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 2)                 20        \n",
      "=================================================================\n",
      "Total params: 2,297,030\n",
      "Trainable params: 2,297,030\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defining our DEC model\n",
    "dec_model = DEC_Agglomerative([2048, 500, 500, 2000, 10], n_clusters=2)\n",
    "dec_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_save_dir = 'results/single_link_agglomerative_clustering'\n",
    "if not exist_directory(results_save_dir):\n",
    "    os.makedirs(results_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Pretraining...\n",
      "Epoch 1/100\n",
      "59968/59968 [==============================] - 5s 82us/step - loss: 0.0151\n",
      "        |==>  acc: 0.5670,  nmi: 0.0622  <==|\n",
      "Epoch 2/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0081\n",
      "Epoch 3/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0071\n",
      "Epoch 4/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0067\n",
      "Epoch 5/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0066\n",
      "Epoch 6/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0064\n",
      "Epoch 7/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0063\n",
      "Epoch 8/100\n",
      "59968/59968 [==============================] - 4s 69us/step - loss: 0.0062\n",
      "Epoch 9/100\n",
      "59968/59968 [==============================] - 4s 70us/step - loss: 0.0061\n",
      "Epoch 10/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0061\n",
      "Epoch 11/100\n",
      "59968/59968 [==============================] - 4s 71us/step - loss: 0.0060\n",
      "        |==>  acc: 0.6710,  nmi: 0.0785  <==|\n",
      "Epoch 12/100\n",
      "59968/59968 [==============================] - 4s 72us/step - loss: 0.0060\n",
      "Epoch 13/100\n",
      "59968/59968 [==============================] - 4s 71us/step - loss: 0.0059\n",
      "Epoch 14/100\n",
      "59968/59968 [==============================] - 4s 71us/step - loss: 0.0059\n",
      "Epoch 15/100\n",
      "59968/59968 [==============================] - 4s 70us/step - loss: 0.0058\n",
      "Epoch 16/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0058\n",
      "Epoch 17/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0058\n",
      "Epoch 18/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0057\n",
      "Epoch 19/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0057\n",
      "Epoch 20/100\n",
      "59968/59968 [==============================] - 4s 70us/step - loss: 0.0057\n",
      "Epoch 21/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0056\n",
      "        |==>  acc: 0.7209,  nmi: 0.1453  <==|\n",
      "Epoch 22/100\n",
      "59968/59968 [==============================] - 5s 76us/step - loss: 0.0056\n",
      "Epoch 23/100\n",
      "59968/59968 [==============================] - 4s 75us/step - loss: 0.0056\n",
      "Epoch 24/100\n",
      "59968/59968 [==============================] - 4s 74us/step - loss: 0.0056\n",
      "Epoch 25/100\n",
      "59968/59968 [==============================] - 4s 73us/step - loss: 0.0055\n",
      "Epoch 26/100\n",
      "59968/59968 [==============================] - 4s 70us/step - loss: 0.0055\n",
      "Epoch 27/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0055\n",
      "Epoch 28/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0055\n",
      "Epoch 29/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0055\n",
      "Epoch 30/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0055\n",
      "Epoch 31/100\n",
      "59968/59968 [==============================] - 4s 70us/step - loss: 0.0054\n",
      "        |==>  acc: 0.6775,  nmi: 0.1014  <==|\n",
      "Epoch 32/100\n",
      "59968/59968 [==============================] - 4s 69us/step - loss: 0.0054\n",
      "Epoch 33/100\n",
      "59968/59968 [==============================] - 4s 69us/step - loss: 0.0054\n",
      "Epoch 34/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0054\n",
      "Epoch 35/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0054\n",
      "Epoch 36/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0054\n",
      "Epoch 37/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0053\n",
      "Epoch 38/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0053\n",
      "Epoch 39/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0053\n",
      "Epoch 40/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0053\n",
      "Epoch 41/100\n",
      "59968/59968 [==============================] - 4s 69us/step - loss: 0.0053\n",
      "        |==>  acc: 0.6873,  nmi: 0.1123  <==|\n",
      "Epoch 42/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0053\n",
      "Epoch 43/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0053\n",
      "Epoch 44/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0052\n",
      "Epoch 45/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0052\n",
      "Epoch 46/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0052\n",
      "Epoch 47/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0052\n",
      "Epoch 48/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0052\n",
      "Epoch 49/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0052\n",
      "Epoch 50/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "Epoch 51/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0051\n",
      "        |==>  acc: 0.6758,  nmi: 0.0819  <==|\n",
      "Epoch 52/100\n",
      "59968/59968 [==============================] - 4s 70us/step - loss: 0.0051\n",
      "Epoch 53/100\n",
      "59968/59968 [==============================] - 4s 69us/step - loss: 0.0051\n",
      "Epoch 54/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0051\n",
      "Epoch 55/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0051\n",
      "Epoch 56/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 57/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 58/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 59/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 60/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 61/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "        |==>  acc: 0.6849,  nmi: 0.1072  <==|\n",
      "Epoch 62/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 63/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0050\n",
      "Epoch 64/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0050\n",
      "Epoch 65/100\n",
      "59968/59968 [==============================] - 4s 69us/step - loss: 0.0050\n",
      "Epoch 66/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0050\n",
      "Epoch 67/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 68/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 69/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 70/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 71/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "        |==>  acc: 0.6636,  nmi: 0.0873  <==|\n",
      "Epoch 72/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0049\n",
      "Epoch 73/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0049\n",
      "Epoch 74/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0049\n",
      "Epoch 75/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 76/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0049\n",
      "Epoch 77/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 78/100\n",
      "52736/59968 [=========================>....] - ETA: 0s - loss: 0.0049"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dec_model.pretrain(train_features, train_labels, epochs=100, save_dir=results_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 140\n",
      "Save interval 9370\n",
      "Initializing cluster centers with k-means.\n",
      "Iter 0: acc = 0.56849, nmi = 0.00075, ari = -0.00006  ; loss= 0\n",
      "saving model to: results/single_link_agglomerative_clustering/DEC_model_0.h5\n",
      "Iter 140: acc = 0.56328, nmi = 0.00476, ari = -0.00214  ; loss= 0.00215\n",
      "Iter 280: acc = 0.54769, nmi = 0.00539, ari = -0.00611  ; loss= 0.00428\n",
      "Iter 420: acc = 0.53095, nmi = 0.00630, ari = -0.00803  ; loss= 0.00817\n",
      "Iter 560: acc = 0.51863, nmi = 0.00616, ari = -0.00773  ; loss= 0.02923\n",
      "Iter 700: acc = 0.51107, nmi = 0.00679, ari = -0.00734  ; loss= 0.02866\n",
      "Iter 840: acc = 0.50664, nmi = 0.00784, ari = -0.00726  ; loss= 0.02808\n",
      "Iter 980: acc = 0.50255, nmi = 0.00832, ari = -0.00683  ; loss= 0.04376\n",
      "Iter 1120: acc = 0.50462, nmi = 0.00870, ari = -0.00560  ; loss= 0.03115\n",
      "Iter 1260: acc = 0.51242, nmi = 0.00991, ari = -0.00419  ; loss= 0.04727\n",
      "Iter 1400: acc = 0.51469, nmi = 0.00937, ari = -0.00335  ; loss= 0.02691\n",
      "Iter 1540: acc = 0.51743, nmi = 0.00871, ari = -0.00228  ; loss= 0.03349\n",
      "Iter 1680: acc = 0.52041, nmi = 0.00903, ari = -0.00150  ; loss= 0.03944\n",
      "Iter 1820: acc = 0.52193, nmi = 0.00863, ari = -0.00085  ; loss= 0.04207\n",
      "Iter 1960: acc = 0.52315, nmi = 0.00898, ari = -0.00060  ; loss= 0.03427\n",
      "Iter 2100: acc = 0.52546, nmi = 0.00873, ari = 0.00028  ; loss= 0.03189\n",
      "Iter 2240: acc = 0.52863, nmi = 0.00917, ari = 0.00123  ; loss= 0.04397\n",
      "Iter 2380: acc = 0.53527, nmi = 0.01011, ari = 0.00343  ; loss= 0.05066\n",
      "Iter 2520: acc = 0.53739, nmi = 0.00873, ari = 0.00468  ; loss= 0.042\n",
      "Iter 2660: acc = 0.53835, nmi = 0.00846, ari = 0.00514  ; loss= 0.04952\n",
      "Iter 2800: acc = 0.53754, nmi = 0.00766, ari = 0.00504  ; loss= 0.03225\n",
      "Iter 2940: acc = 0.53809, nmi = 0.00731, ari = 0.00533  ; loss= 0.04828\n",
      "Iter 3080: acc = 0.54060, nmi = 0.00749, ari = 0.00626  ; loss= 0.02288\n",
      "Iter 3220: acc = 0.54456, nmi = 0.00836, ari = 0.00767  ; loss= 0.0576\n",
      "Iter 3360: acc = 0.54311, nmi = 0.00801, ari = 0.00715  ; loss= 0.03245\n",
      "Iter 3500: acc = 0.53817, nmi = 0.00672, ari = 0.00550  ; loss= 0.10732\n",
      "Iter 3640: acc = 0.55169, nmi = 0.00916, ari = 0.01062  ; loss= 0.03741\n",
      "Iter 3780: acc = 0.55254, nmi = 0.00932, ari = 0.01098  ; loss= 0.20677\n",
      "Iter 3920: acc = 0.55386, nmi = 0.00948, ari = 0.01156  ; loss= 0.02166\n",
      "Iter 4060: acc = 0.55159, nmi = 0.00913, ari = 0.01058  ; loss= 0.05196\n",
      "Iter 4200: acc = 0.55209, nmi = 0.00982, ari = 0.01073  ; loss= 0.0195\n",
      "Iter 4340: acc = 0.55513, nmi = 0.01134, ari = 0.01199  ; loss= 0.02434\n",
      "Iter 4480: acc = 0.55728, nmi = 0.01188, ari = 0.01298  ; loss= 0.01576\n",
      "Iter 4620: acc = 0.55470, nmi = 0.01129, ari = 0.01178  ; loss= 0.04995\n",
      "Iter 4760: acc = 0.55391, nmi = 0.01024, ari = 0.01153  ; loss= 0.03685\n",
      "Iter 4900: acc = 0.55311, nmi = 0.00993, ari = 0.01119  ; loss= 0.0175\n",
      "Iter 5040: acc = 0.56625, nmi = 0.01356, ari = 0.01753  ; loss= 0.02175\n",
      "Iter 5180: acc = 0.56442, nmi = 0.01207, ari = 0.01658  ; loss= 0.01718\n",
      "Iter 5320: acc = 0.56178, nmi = 0.01059, ari = 0.01524  ; loss= 0.04443\n",
      "Iter 5460: acc = 0.56113, nmi = 0.00972, ari = 0.01487  ; loss= 0.0126\n",
      "Iter 5600: acc = 0.56085, nmi = 0.00980, ari = 0.01475  ; loss= 0.02665\n",
      "Iter 5740: acc = 0.56013, nmi = 0.00930, ari = 0.01438  ; loss= 0.14324\n",
      "Iter 5880: acc = 0.55343, nmi = 0.00706, ari = 0.01131  ; loss= 0.03094\n",
      "Iter 6020: acc = 0.54931, nmi = 0.00631, ari = 0.00967  ; loss= 0.01678\n",
      "Iter 6160: acc = 0.55023, nmi = 0.00645, ari = 0.01002  ; loss= 0.01674\n",
      "Iter 6300: acc = 0.54466, nmi = 0.00543, ari = 0.00795  ; loss= 0.20749\n",
      "Iter 6440: acc = 0.54472, nmi = 0.00554, ari = 0.00798  ; loss= 0.01215\n",
      "Iter 6580: acc = 0.54059, nmi = 0.00417, ari = 0.00653  ; loss= 0.14188\n",
      "Iter 6720: acc = 0.54015, nmi = 0.00439, ari = 0.00642  ; loss= 0.01192\n",
      "Iter 6860: acc = 0.53855, nmi = 0.00457, ari = 0.00593  ; loss= 0.01838\n",
      "Iter 7000: acc = 0.53869, nmi = 0.00447, ari = 0.00597  ; loss= 0.01194\n",
      "Iter 7140: acc = 0.53889, nmi = 0.00482, ari = 0.00602  ; loss= 0.01776\n",
      "Iter 7280: acc = 0.53920, nmi = 0.00554, ari = 0.00607  ; loss= 0.08601\n",
      "Iter 7420: acc = 0.54040, nmi = 0.00571, ari = 0.00647  ; loss= 0.01681\n",
      "Iter 7560: acc = 0.54146, nmi = 0.00642, ari = 0.00676  ; loss= 0.01338\n",
      "Iter 7700: acc = 0.53932, nmi = 0.00543, ari = 0.00612  ; loss= 0.01708\n",
      "Iter 7840: acc = 0.54512, nmi = 0.00665, ari = 0.00811  ; loss= 0.01469\n",
      "Iter 7980: acc = 0.54089, nmi = 0.00547, ari = 0.00665  ; loss= 0.0285\n",
      "Iter 8120: acc = 0.53795, nmi = 0.00497, ari = 0.00571  ; loss= 0.0441\n",
      "Iter 8260: acc = 0.53805, nmi = 0.00521, ari = 0.00572  ; loss= 0.01231\n",
      "Iter 8400: acc = 0.53654, nmi = 0.00467, ari = 0.00529  ; loss= 0.01239\n",
      "Iter 8540: acc = 0.53572, nmi = 0.00465, ari = 0.00503  ; loss= 0.02486\n",
      "Iter 8680: acc = 0.53849, nmi = 0.00532, ari = 0.00585  ; loss= 0.01259\n",
      "Iter 8820: acc = 0.53855, nmi = 0.00590, ari = 0.00580  ; loss= 0.01879\n",
      "Iter 8960: acc = 0.53812, nmi = 0.00553, ari = 0.00570  ; loss= 0.00985\n",
      "Iter 9100: acc = 0.53792, nmi = 0.00503, ari = 0.00569  ; loss= 0.02209\n",
      "Iter 9240: acc = 0.53802, nmi = 0.00480, ari = 0.00575  ; loss= 0.01326\n",
      "saving model to: results/single_link_agglomerative_clustering/DEC_model_9370.h5\n",
      "Iter 9380: acc = 0.53670, nmi = 0.00447, ari = 0.00535  ; loss= 0.01667\n",
      "Iter 9520: acc = 0.53552, nmi = 0.00407, ari = 0.00502  ; loss= 0.00947\n",
      "Iter 9660: acc = 0.53507, nmi = 0.00369, ari = 0.00490  ; loss= 0.05356\n",
      "Iter 9800: acc = 0.53525, nmi = 0.00359, ari = 0.00495  ; loss= 0.01059\n",
      "Iter 9940: acc = 0.53487, nmi = 0.00290, ari = 0.00479  ; loss= 0.01281\n",
      "Iter 10080: acc = 0.53862, nmi = 0.00338, ari = 0.00585  ; loss= 0.01906\n",
      "Iter 10220: acc = 0.53989, nmi = 0.00331, ari = 0.00617  ; loss= 0.01891\n",
      "Iter 10360: acc = 0.53834, nmi = 0.00304, ari = 0.00570  ; loss= 0.02746\n",
      "Iter 10500: acc = 0.53822, nmi = 0.00302, ari = 0.00566  ; loss= 0.01881\n",
      "Iter 10640: acc = 0.54019, nmi = 0.00318, ari = 0.00621  ; loss= 0.01246\n",
      "Iter 10780: acc = 0.54217, nmi = 0.00294, ari = 0.00660  ; loss= 0.01895\n",
      "Iter 10920: acc = 0.54066, nmi = 0.00280, ari = 0.00617  ; loss= 0.016\n",
      "Iter 11060: acc = 0.54507, nmi = 0.00480, ari = 0.00801  ; loss= 0.02354\n",
      "Iter 11200: acc = 0.54726, nmi = 0.00544, ari = 0.00884  ; loss= 0.01528\n",
      "Iter 11340: acc = 0.54547, nmi = 0.00480, ari = 0.00814  ; loss= 0.01126\n",
      "Iter 11480: acc = 0.53985, nmi = 0.00352, ari = 0.00622  ; loss= 0.01477\n",
      "Iter 11620: acc = 0.54256, nmi = 0.00410, ari = 0.00711  ; loss= 0.01155\n",
      "Iter 11760: acc = 0.53882, nmi = 0.00302, ari = 0.00581  ; loss= 0.00935\n",
      "Iter 11900: acc = 0.53824, nmi = 0.00289, ari = 0.00562  ; loss= 0.01301\n",
      "Iter 12040: acc = 0.53273, nmi = 0.00171, ari = 0.00395  ; loss= 0.14491\n",
      "Iter 12180: acc = 0.53504, nmi = 0.00181, ari = 0.00444  ; loss= 0.01587\n",
      "Iter 12320: acc = 0.53247, nmi = 0.00140, ari = 0.00371  ; loss= 0.00972\n",
      "Iter 12460: acc = 0.53303, nmi = 0.00151, ari = 0.00388  ; loss= 0.00825\n",
      "Iter 12600: acc = 0.53358, nmi = 0.00165, ari = 0.00407  ; loss= 0.0111\n",
      "Iter 12740: acc = 0.53400, nmi = 0.00201, ari = 0.00434  ; loss= 0.0282\n",
      "Iter 12880: acc = 0.53340, nmi = 0.00179, ari = 0.00412  ; loss= 0.00885\n",
      "Iter 13020: acc = 0.53173, nmi = 0.00159, ari = 0.00370  ; loss= 0.00896\n",
      "Iter 13160: acc = 0.53320, nmi = 0.00156, ari = 0.00395  ; loss= 0.02818\n",
      "Iter 13300: acc = 0.53108, nmi = 0.00106, ari = 0.00321  ; loss= 0.04008\n",
      "Iter 13440: acc = 0.53145, nmi = 0.00086, ari = 0.00305  ; loss= 0.04194\n",
      "Iter 13580: acc = 0.53153, nmi = 0.00074, ari = 0.00289  ; loss= 0.00881\n",
      "Iter 13720: acc = 0.52978, nmi = 0.00077, ari = 0.00274  ; loss= 0.01125\n",
      "Iter 13860: acc = 0.52928, nmi = 0.00070, ari = 0.00259  ; loss= 0.01658\n",
      "Iter 14000: acc = 0.52890, nmi = 0.00059, ari = 0.00240  ; loss= 0.05007\n",
      "Iter 14140: acc = 0.52933, nmi = 0.00081, ari = 0.00273  ; loss= 0.07952\n",
      "Iter 14280: acc = 0.52978, nmi = 0.00084, ari = 0.00282  ; loss= 0.01004\n",
      "Iter 14420: acc = 0.52927, nmi = 0.00082, ari = 0.00273  ; loss= 0.00773\n",
      "Iter 14560: acc = 0.53345, nmi = 0.00110, ari = 0.00359  ; loss= 0.04471\n",
      "Iter 14700: acc = 0.53540, nmi = 0.00124, ari = 0.00402  ; loss= 0.01277\n",
      "Iter 14840: acc = 0.53655, nmi = 0.00142, ari = 0.00439  ; loss= 0.00801\n",
      "Iter 14980: acc = 0.53945, nmi = 0.00185, ari = 0.00528  ; loss= 0.00988\n",
      "Iter 15120: acc = 0.53680, nmi = 0.00157, ari = 0.00457  ; loss= 0.01744\n",
      "Iter 15260: acc = 0.53602, nmi = 0.00178, ari = 0.00460  ; loss= 0.02268\n",
      "Iter 15400: acc = 0.53432, nmi = 0.00155, ari = 0.00413  ; loss= 0.0139\n",
      "Iter 15540: acc = 0.53367, nmi = 0.00177, ari = 0.00416  ; loss= 0.0078\n",
      "Iter 15680: acc = 0.53213, nmi = 0.00115, ari = 0.00345  ; loss= 0.05708\n",
      "Iter 15820: acc = 0.53292, nmi = 0.00126, ari = 0.00367  ; loss= 0.00613\n",
      "Iter 15960: acc = 0.53187, nmi = 0.00115, ari = 0.00341  ; loss= 0.00739\n",
      "Iter 16100: acc = 0.53148, nmi = 0.00129, ari = 0.00347  ; loss= 0.05738\n",
      "Iter 16240: acc = 0.52983, nmi = 0.00121, ari = 0.00315  ; loss= 0.02298\n",
      "Iter 16380: acc = 0.52903, nmi = 0.00116, ari = 0.00300  ; loss= 0.00709\n",
      "Iter 16520: acc = 0.52835, nmi = 0.00109, ari = 0.00285  ; loss= 0.00799\n",
      "Iter 16660: acc = 0.52838, nmi = 0.00106, ari = 0.00283  ; loss= 0.03827\n",
      "Iter 16800: acc = 0.52888, nmi = 0.00111, ari = 0.00294  ; loss= 0.00714\n",
      "Iter 16940: acc = 0.52893, nmi = 0.00114, ari = 0.00297  ; loss= 0.03462\n",
      "Iter 17080: acc = 0.52725, nmi = 0.00089, ari = 0.00254  ; loss= 0.01076\n",
      "Iter 17220: acc = 0.52751, nmi = 0.00099, ari = 0.00265  ; loss= 0.00738\n",
      "Iter 17360: acc = 0.52700, nmi = 0.00095, ari = 0.00256  ; loss= 0.00847\n",
      "Iter 17500: acc = 0.53000, nmi = 0.00131, ari = 0.00325  ; loss= 0.00746\n",
      "Iter 17640: acc = 0.53093, nmi = 0.00160, ari = 0.00356  ; loss= 0.00847\n",
      "Iter 17780: acc = 0.52528, nmi = 0.00209, ari = 0.00253  ; loss= 0.3484\n",
      "Iter 17920: acc = 0.52628, nmi = 0.00204, ari = 0.00275  ; loss= 0.00718\n",
      "Iter 18060: acc = 0.52605, nmi = 0.00211, ari = 0.00270  ; loss= 0.00841\n",
      "Iter 18200: acc = 0.52731, nmi = 0.00215, ari = 0.00297  ; loss= 0.02951\n",
      "Iter 18340: acc = 0.52498, nmi = 0.00202, ari = 0.00247  ; loss= 0.01945\n",
      "Iter 18480: acc = 0.52610, nmi = 0.00225, ari = 0.00270  ; loss= 0.00656\n",
      "Iter 18620: acc = 0.52558, nmi = 0.00225, ari = 0.00259  ; loss= 0.00687\n",
      "saving model to: results/single_link_agglomerative_clustering/DEC_model_18740.h5\n",
      "Iter 18760: acc = 0.52561, nmi = 0.00199, ari = 0.00261  ; loss= 0.00845\n",
      "Iter 18900: acc = 0.52455, nmi = 0.00169, ari = 0.00239  ; loss= 0.02218\n",
      "Iter 19040: acc = 0.52576, nmi = 0.00174, ari = 0.00263  ; loss= 0.04373\n",
      "Iter 19180: acc = 0.52461, nmi = 0.00156, ari = 0.00240  ; loss= 0.03047\n",
      "Iter 19320: acc = 0.52400, nmi = 0.00158, ari = 0.00228  ; loss= 0.00846\n",
      "Iter 19460: acc = 0.52221, nmi = 0.00150, ari = 0.00196  ; loss= 0.02128\n",
      "Iter 19600: acc = 0.51838, nmi = 0.00131, ari = 0.00131  ; loss= 0.01839\n",
      "Iter 19740: acc = 0.52049, nmi = 0.00142, ari = 0.00166  ; loss= 0.00611\n",
      "Iter 19880: acc = 0.51949, nmi = 0.00138, ari = 0.00149  ; loss= 0.00783\n",
      "saving model to: results/single_link_agglomerative_clustering/DEC_model_final.h5\n",
      "acc: 0.5194937299893276\n",
      "CPU times: user 11min 41s, sys: 1min 43s, total: 13min 24s\n",
      "Wall time: 10min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dec_model.compile(optimizer=SGD(0.01, 0.9), loss='kld')\n",
    "y_pred = dec_model.fit(train_features, y=train_labels, batch_size=32, save_dir=results_save_dir, \n",
    "                       cluster_linkage='single')\n",
    "print('acc:', metrics.acc(train_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the features extracted by the DEC model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Output Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dec_encoder_model_pred = dec_model.encoder.predict(train_features)\n",
    "pca_mod = PCA(2)\n",
    "pca_mod.fit(dec_encoder_model_pred)\n",
    "reduced_features = pca_mod.transform(dec_encoder_model_pred)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.scatterplot(reduced_features[:, 0], reduced_features[:, 1], \n",
    "                hue=np.where(train_labels==0, 'elliptical', 'spiral'), ax=ax)\n",
    "ax.text(0.02, 0.92, f'Explained Var: {np.round(np.sum(pca_mod.explained_variance_ratio_), decimals=4)}', \n",
    "        transform=ax.transAxes)\n",
    "ax.set_xlabel('PCA Dim 1')\n",
    "ax.set_ylabel('PCA Dim 2')\n",
    "ax.set_title('PCA Scatterplot for Encoder Features')\n",
    "ax.legend(loc=4)\n",
    "plt.savefig(f'{results_save_dir}/PCAencoding_pca_features.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_log, ptrain_log = pd.read_csv(f'{results_save_dir}/dec_log.csv'), pd.read_csv(f'{results_save_dir}/pretrain_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7105afd08158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mptrain_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Reconstruction Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdec_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Clustering Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdec_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nmi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Clustering NMI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{results_save_dir}/training_curves.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "ptrain_log.plot(x='epoch', y='loss', ax=ax[0], title='Reconstruction Loss')\n",
    "dec_log.plot(x='iter', y='acc', ax=ax[1], title='Clustering Accuracy')\n",
    "dec_log.plot(x='iter', y='nmi', ax=ax[-1], title='Clustering NMI')\n",
    "plt.savefig(f'{results_save_dir}/training_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete-Link Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEC Xception Training Regime\n",
    "\n",
    "This part of the notebook defines the generator for the training regime of the DEC model over the features extracted from the Xception architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 500)               1024500   \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "encoder_2 (Dense)            (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "encoder_3 (Dense)            (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 2)                 20        \n",
      "=================================================================\n",
      "Total params: 2,297,030\n",
      "Trainable params: 2,297,030\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defining our DEC model\n",
    "dec_model = DEC_Agglomerative([2048, 500, 500, 2000, 10], n_clusters=2)\n",
    "dec_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_save_dir = 'results/complete_link_agglomerative_clustering'\n",
    "if not exist_directory(results_save_dir):\n",
    "    os.makedirs(results_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Pretraining...\n",
      "Epoch 1/100\n",
      "59968/59968 [==============================] - 5s 82us/step - loss: 0.0156\n",
      "        |==>  acc: 0.6630,  nmi: 0.0757  <==|\n",
      "Epoch 2/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0081\n",
      "Epoch 3/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0071\n",
      "Epoch 4/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0067\n",
      "Epoch 5/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0065\n",
      "Epoch 6/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0064\n",
      "Epoch 7/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0063\n",
      "Epoch 8/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0062\n",
      "Epoch 9/100\n",
      "59968/59968 [==============================] - 4s 64us/step - loss: 0.0061\n",
      "Epoch 10/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0060\n",
      "Epoch 11/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0060\n",
      "        |==>  acc: 0.6426,  nmi: 0.0646  <==|\n",
      "Epoch 12/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0059\n",
      "Epoch 13/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0059\n",
      "Epoch 14/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0059\n",
      "Epoch 15/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0058\n",
      "Epoch 16/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0058\n",
      "Epoch 17/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0057\n",
      "Epoch 18/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0057\n",
      "Epoch 19/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0057\n",
      "Epoch 20/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0057\n",
      "Epoch 21/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0056\n",
      "        |==>  acc: 0.5857,  nmi: 0.0173  <==|\n",
      "Epoch 22/100\n",
      "59968/59968 [==============================] - 4s 71us/step - loss: 0.0056\n",
      "Epoch 23/100\n",
      "59968/59968 [==============================] - 4s 69us/step - loss: 0.0056\n",
      "Epoch 24/100\n",
      "59968/59968 [==============================] - 4s 69us/step - loss: 0.0056\n",
      "Epoch 25/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0055\n",
      "Epoch 26/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0055\n",
      "Epoch 27/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0055\n",
      "Epoch 28/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0055\n",
      "Epoch 29/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0055\n",
      "Epoch 30/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0054\n",
      "Epoch 31/100\n",
      "59968/59968 [==============================] - 4s 69us/step - loss: 0.0054\n",
      "        |==>  acc: 0.6558,  nmi: 0.0616  <==|\n",
      "Epoch 32/100\n",
      "59968/59968 [==============================] - 4s 74us/step - loss: 0.0054\n",
      "Epoch 33/100\n",
      "59968/59968 [==============================] - 4s 72us/step - loss: 0.0054\n",
      "Epoch 34/100\n",
      "59968/59968 [==============================] - 4s 72us/step - loss: 0.0054\n",
      "Epoch 35/100\n",
      "59968/59968 [==============================] - 4s 72us/step - loss: 0.0054\n",
      "Epoch 36/100\n",
      "59968/59968 [==============================] - 4s 72us/step - loss: 0.0054\n",
      "Epoch 37/100\n",
      "59968/59968 [==============================] - 4s 69us/step - loss: 0.0053\n",
      "Epoch 38/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0053\n",
      "Epoch 39/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0053\n",
      "Epoch 40/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0053\n",
      "Epoch 41/100\n",
      "59968/59968 [==============================] - 4s 70us/step - loss: 0.0053\n",
      "        |==>  acc: 0.6745,  nmi: 0.0940  <==|\n",
      "Epoch 42/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0053\n",
      "Epoch 43/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0052\n",
      "Epoch 44/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0052\n",
      "Epoch 45/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0052\n",
      "Epoch 46/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0052\n",
      "Epoch 47/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0052\n",
      "Epoch 48/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0052\n",
      "Epoch 49/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0052\n",
      "Epoch 50/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0052\n",
      "Epoch 51/100\n",
      "59968/59968 [==============================] - 4s 69us/step - loss: 0.0051\n",
      "        |==>  acc: 0.6775,  nmi: 0.0952  <==|\n",
      "Epoch 52/100\n",
      "59968/59968 [==============================] - 4s 71us/step - loss: 0.0051\n",
      "Epoch 53/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 54/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 55/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 56/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 57/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 58/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 59/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0051\n",
      "Epoch 60/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0050\n",
      "Epoch 61/100\n",
      "59968/59968 [==============================] - 4s 69us/step - loss: 0.0050\n",
      "        |==>  acc: 0.7230,  nmi: 0.1481  <==|\n",
      "Epoch 62/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0050\n",
      "Epoch 63/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0050\n",
      "Epoch 64/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0050\n",
      "Epoch 65/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0050\n",
      "Epoch 66/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0050\n",
      "Epoch 67/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0050\n",
      "Epoch 68/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 69/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 70/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 71/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "        |==>  acc: 0.6611,  nmi: 0.0884  <==|\n",
      "Epoch 72/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0049\n",
      "Epoch 73/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 74/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0049\n",
      "Epoch 75/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0049\n",
      "Epoch 76/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0049\n",
      "Epoch 77/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0049\n",
      "Epoch 78/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 79/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 80/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 81/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "        |==>  acc: 0.6728,  nmi: 0.0850  <==|\n",
      "Epoch 82/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0049\n",
      "Epoch 83/100\n",
      "59968/59968 [==============================] - 4s 64us/step - loss: 0.0049\n",
      "Epoch 84/100\n",
      "59968/59968 [==============================] - 4s 64us/step - loss: 0.0048\n",
      "Epoch 85/100\n",
      "59968/59968 [==============================] - 4s 64us/step - loss: 0.0049\n",
      "Epoch 86/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 87/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 88/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 89/100\n",
      "59968/59968 [==============================] - 4s 64us/step - loss: 0.0048\n",
      "Epoch 90/100\n",
      "59968/59968 [==============================] - 4s 64us/step - loss: 0.0048\n",
      "Epoch 91/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "        |==>  acc: 0.6324,  nmi: 0.0416  <==|\n",
      "Epoch 92/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0048\n",
      "Epoch 93/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0048\n",
      "Epoch 94/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 95/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0048\n",
      "Epoch 96/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0048\n",
      "Epoch 97/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 98/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0048\n",
      "Epoch 99/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0048\n",
      "Epoch 100/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0047\n",
      "Pretraining time: 1716s\n",
      "Pretrained weights are saved to results/complete_link_agglomerative_clustering/ae_weights.h5\n",
      "CPU times: user 26min 59s, sys: 2min 27s, total: 29min 27s\n",
      "Wall time: 28min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dec_model.pretrain(train_features, train_labels, epochs=100, save_dir=results_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 140\n",
      "Save interval 9370\n",
      "Initializing cluster centers with k-means.\n",
      "Iter 0: acc = 0.56844, nmi = 0.00197, ari = -0.00009  ; loss= 0\n",
      "saving model to: results/complete_link_agglomerative_clustering/DEC_model_0.h5\n",
      "Iter 140: acc = 0.56527, nmi = 0.00269, ari = -0.00131  ; loss= 0.00241\n",
      "Iter 280: acc = 0.55636, nmi = 0.00153, ari = -0.00313  ; loss= 0.00419\n",
      "Iter 420: acc = 0.54556, nmi = 0.00143, ari = -0.00426  ; loss= 0.00819\n",
      "Iter 560: acc = 0.53962, nmi = 0.00088, ari = -0.00372  ; loss= 0.03223\n",
      "Iter 700: acc = 0.53467, nmi = 0.00088, ari = -0.00373  ; loss= 0.0176\n",
      "Iter 840: acc = 0.53037, nmi = 0.00092, ari = -0.00371  ; loss= 0.02774\n",
      "Iter 980: acc = 0.52745, nmi = 0.00072, ari = -0.00321  ; loss= 0.04192\n",
      "Iter 1120: acc = 0.52396, nmi = 0.00070, ari = -0.00299  ; loss= 0.03646\n",
      "Iter 1260: acc = 0.51939, nmi = 0.00113, ari = -0.00349  ; loss= 0.04609\n",
      "Iter 1400: acc = 0.51733, nmi = 0.00106, ari = -0.00321  ; loss= 0.0272\n",
      "Iter 1540: acc = 0.51679, nmi = 0.00095, ari = -0.00298  ; loss= 0.02659\n",
      "Iter 1680: acc = 0.51571, nmi = 0.00105, ari = -0.00305  ; loss= 0.04221\n",
      "Iter 1820: acc = 0.51891, nmi = 0.00056, ari = -0.00237  ; loss= 0.04596\n",
      "Iter 1960: acc = 0.52248, nmi = 0.00015, ari = -0.00127  ; loss= 0.03598\n",
      "Iter 2100: acc = 0.51976, nmi = 0.00028, ari = -0.00167  ; loss= 0.03042\n",
      "Iter 2240: acc = 0.52278, nmi = 0.00002, ari = -0.00051  ; loss= 0.04253\n",
      "Iter 2380: acc = 0.52068, nmi = 0.00009, ari = -0.00092  ; loss= 0.03712\n",
      "Iter 2520: acc = 0.52240, nmi = 0.00003, ari = -0.00055  ; loss= 0.03957\n",
      "Iter 2660: acc = 0.51998, nmi = 0.00006, ari = -0.00076  ; loss= 0.04292\n",
      "Iter 2800: acc = 0.51914, nmi = 0.00013, ari = -0.00109  ; loss= 0.04407\n",
      "Iter 2940: acc = 0.51726, nmi = 0.00016, ari = -0.00113  ; loss= 0.02831\n",
      "Iter 3080: acc = 0.51573, nmi = 0.00011, ari = -0.00085  ; loss= 0.03358\n",
      "Iter 3220: acc = 0.51694, nmi = 0.00004, ari = -0.00050  ; loss= 0.01572\n",
      "Iter 3360: acc = 0.51633, nmi = 0.00002, ari = -0.00041  ; loss= 0.02473\n",
      "Iter 3500: acc = 0.51521, nmi = 0.00005, ari = -0.00054  ; loss= 0.02152\n",
      "Iter 3640: acc = 0.51231, nmi = 0.00012, ari = -0.00076  ; loss= 0.04524\n",
      "Iter 3780: acc = 0.51296, nmi = 0.00011, ari = -0.00076  ; loss= 0.01745\n",
      "Iter 3920: acc = 0.51959, nmi = 0.00000, ari = -0.00004  ; loss= 0.02019\n",
      "Iter 4060: acc = 0.51933, nmi = 0.00000, ari = 0.00008  ; loss= 0.02174\n",
      "Iter 4200: acc = 0.52026, nmi = 0.00001, ari = 0.00018  ; loss= 0.0231\n",
      "Iter 4340: acc = 0.52316, nmi = 0.00019, ari = 0.00117  ; loss= 0.0245\n",
      "Iter 4480: acc = 0.52555, nmi = 0.00023, ari = 0.00143  ; loss= 0.01457\n",
      "Iter 4620: acc = 0.52595, nmi = 0.00028, ari = 0.00157  ; loss= 0.02156\n",
      "Iter 4760: acc = 0.52611, nmi = 0.00043, ari = 0.00188  ; loss= 0.01441\n",
      "Iter 4900: acc = 0.53133, nmi = 0.00057, ari = 0.00261  ; loss= 0.03255\n",
      "Iter 5040: acc = 0.53150, nmi = 0.00061, ari = 0.00269  ; loss= 0.02253\n",
      "Iter 5180: acc = 0.53028, nmi = 0.00061, ari = 0.00257  ; loss= 0.01807\n",
      "Iter 5320: acc = 0.53248, nmi = 0.00090, ari = 0.00323  ; loss= 0.08371\n",
      "Iter 5460: acc = 0.53333, nmi = 0.00117, ari = 0.00365  ; loss= 0.11961\n",
      "Iter 5600: acc = 0.53105, nmi = 0.00088, ari = 0.00302  ; loss= 0.01935\n",
      "Iter 5740: acc = 0.53475, nmi = 0.00181, ari = 0.00438  ; loss= 0.07176\n",
      "Iter 5880: acc = 0.53222, nmi = 0.00150, ari = 0.00373  ; loss= 0.03828\n",
      "Iter 6020: acc = 0.53405, nmi = 0.00195, ari = 0.00432  ; loss= 0.0442\n",
      "Iter 6160: acc = 0.53577, nmi = 0.00227, ari = 0.00483  ; loss= 0.01937\n",
      "Iter 6300: acc = 0.53569, nmi = 0.00194, ari = 0.00464  ; loss= 0.08583\n",
      "Iter 6440: acc = 0.53810, nmi = 0.00189, ari = 0.00507  ; loss= 0.01255\n",
      "Iter 6580: acc = 0.53880, nmi = 0.00215, ari = 0.00539  ; loss= 0.02978\n",
      "Iter 6720: acc = 0.53519, nmi = 0.00148, ari = 0.00422  ; loss= 0.03835\n",
      "Iter 6860: acc = 0.53692, nmi = 0.00185, ari = 0.00482  ; loss= 0.02285\n",
      "Iter 7000: acc = 0.53272, nmi = 0.00138, ari = 0.00374  ; loss= 0.02119\n",
      "Iter 7140: acc = 0.53203, nmi = 0.00143, ari = 0.00366  ; loss= 0.01799\n",
      "Iter 7280: acc = 0.53350, nmi = 0.00188, ari = 0.00418  ; loss= 0.01749\n",
      "Iter 7420: acc = 0.53649, nmi = 0.00246, ari = 0.00506  ; loss= 0.01772\n",
      "Iter 7560: acc = 0.53717, nmi = 0.00251, ari = 0.00523  ; loss= 0.03484\n",
      "Iter 7700: acc = 0.53397, nmi = 0.00210, ari = 0.00437  ; loss= 0.0129\n",
      "Iter 7840: acc = 0.53440, nmi = 0.00208, ari = 0.00445  ; loss= 0.03274\n",
      "Iter 7980: acc = 0.53504, nmi = 0.00205, ari = 0.00457  ; loss= 0.00963\n",
      "Iter 8120: acc = 0.53298, nmi = 0.00199, ari = 0.00413  ; loss= 0.04046\n",
      "Iter 8260: acc = 0.53168, nmi = 0.00180, ari = 0.00379  ; loss= 0.01323\n",
      "Iter 8400: acc = 0.53148, nmi = 0.00185, ari = 0.00377  ; loss= 0.01155\n",
      "Iter 8540: acc = 0.53045, nmi = 0.00176, ari = 0.00354  ; loss= 0.02864\n",
      "Iter 8680: acc = 0.53337, nmi = 0.00207, ari = 0.00423  ; loss= 0.01052\n",
      "Iter 8820: acc = 0.53362, nmi = 0.00197, ari = 0.00424  ; loss= 0.01052\n",
      "Iter 8960: acc = 0.53392, nmi = 0.00193, ari = 0.00429  ; loss= 0.01369\n",
      "Iter 9100: acc = 0.53423, nmi = 0.00211, ari = 0.00443  ; loss= 0.02038\n",
      "Iter 9240: acc = 0.53378, nmi = 0.00220, ari = 0.00437  ; loss= 0.01369\n",
      "saving model to: results/complete_link_agglomerative_clustering/DEC_model_9370.h5\n",
      "Iter 9380: acc = 0.53158, nmi = 0.00200, ari = 0.00384  ; loss= 0.01229\n",
      "Iter 9520: acc = 0.53152, nmi = 0.00201, ari = 0.00383  ; loss= 0.01124\n",
      "Iter 9660: acc = 0.53218, nmi = 0.00209, ari = 0.00400  ; loss= 0.0144\n",
      "Iter 9800: acc = 0.53237, nmi = 0.00268, ari = 0.00415  ; loss= 0.1218\n",
      "Iter 9940: acc = 0.53332, nmi = 0.00261, ari = 0.00437  ; loss= 0.01406\n",
      "Iter 10080: acc = 0.53343, nmi = 0.00288, ari = 0.00443  ; loss= 0.01059\n",
      "Iter 10220: acc = 0.53485, nmi = 0.00317, ari = 0.00482  ; loss= 0.01234\n",
      "Iter 10360: acc = 0.53444, nmi = 0.00323, ari = 0.00472  ; loss= 0.01109\n",
      "Iter 10500: acc = 0.53574, nmi = 0.00337, ari = 0.00508  ; loss= 0.01083\n",
      "Iter 10640: acc = 0.53542, nmi = 0.00293, ari = 0.00494  ; loss= 0.19977\n",
      "Iter 10780: acc = 0.53537, nmi = 0.00268, ari = 0.00487  ; loss= 0.01179\n",
      "Iter 10920: acc = 0.53564, nmi = 0.00287, ari = 0.00498  ; loss= 0.02299\n",
      "Iter 11060: acc = 0.53530, nmi = 0.00250, ari = 0.00480  ; loss= 0.00981\n",
      "Iter 11200: acc = 0.53482, nmi = 0.00233, ari = 0.00464  ; loss= 0.02046\n",
      "Iter 11340: acc = 0.53430, nmi = 0.00207, ari = 0.00443  ; loss= 0.01562\n",
      "Iter 11480: acc = 0.53395, nmi = 0.00205, ari = 0.00435  ; loss= 0.03652\n",
      "Iter 11620: acc = 0.53485, nmi = 0.00220, ari = 0.00460  ; loss= 0.0426\n",
      "Iter 11760: acc = 0.53519, nmi = 0.00267, ari = 0.00482  ; loss= 0.03223\n",
      "Iter 11900: acc = 0.53509, nmi = 0.00253, ari = 0.00477  ; loss= 0.01957\n",
      "Iter 12040: acc = 0.53550, nmi = 0.00274, ari = 0.00492  ; loss= 0.02362\n",
      "Iter 12180: acc = 0.53802, nmi = 0.00277, ari = 0.00553  ; loss= 0.12768\n",
      "Iter 12320: acc = 0.53824, nmi = 0.00281, ari = 0.00560  ; loss= 0.0081\n",
      "Iter 12460: acc = 0.53544, nmi = 0.00233, ari = 0.00477  ; loss= 0.05975\n",
      "Iter 12600: acc = 0.53487, nmi = 0.00215, ari = 0.00458  ; loss= 0.01968\n",
      "Iter 12740: acc = 0.53642, nmi = 0.00243, ari = 0.00503  ; loss= 0.0291\n",
      "Iter 12880: acc = 0.54167, nmi = 0.00309, ari = 0.00655  ; loss= 0.20939\n",
      "Iter 13020: acc = 0.54556, nmi = 0.00341, ari = 0.00768  ; loss= 0.0085\n",
      "Iter 13160: acc = 0.54619, nmi = 0.00367, ari = 0.00798  ; loss= 0.00969\n",
      "Iter 13300: acc = 0.54682, nmi = 0.00391, ari = 0.00826  ; loss= 0.00799\n",
      "Iter 13440: acc = 0.54651, nmi = 0.00373, ari = 0.00809  ; loss= 0.02811\n",
      "Iter 13580: acc = 0.54713, nmi = 0.00407, ari = 0.00842  ; loss= 0.00831\n",
      "Iter 13720: acc = 0.54909, nmi = 0.00438, ari = 0.00912  ; loss= 0.01118\n",
      "Iter 13860: acc = 0.55129, nmi = 0.00480, ari = 0.00995  ; loss= 0.00948\n",
      "Iter 14000: acc = 0.55049, nmi = 0.00491, ari = 0.00975  ; loss= 0.01479\n",
      "Iter 14140: acc = 0.54841, nmi = 0.00454, ari = 0.00898  ; loss= 0.01799\n",
      "Iter 14280: acc = 0.54738, nmi = 0.00451, ari = 0.00866  ; loss= 0.02784\n",
      "Iter 14420: acc = 0.54569, nmi = 0.00378, ari = 0.00789  ; loss= 0.03034\n",
      "Iter 14560: acc = 0.54703, nmi = 0.00400, ari = 0.00836  ; loss= 0.01528\n",
      "Iter 14700: acc = 0.54669, nmi = 0.00386, ari = 0.00820  ; loss= 0.00763\n",
      "Iter 14840: acc = 0.54688, nmi = 0.00398, ari = 0.00831  ; loss= 0.00867\n",
      "Iter 14980: acc = 0.54581, nmi = 0.00372, ari = 0.00790  ; loss= 0.00857\n",
      "Iter 15120: acc = 0.54562, nmi = 0.00376, ari = 0.00787  ; loss= 0.00874\n",
      "Iter 15260: acc = 0.54532, nmi = 0.00392, ari = 0.00785  ; loss= 0.00898\n",
      "Iter 15400: acc = 0.54582, nmi = 0.00405, ari = 0.00804  ; loss= 0.00913\n",
      "Iter 15540: acc = 0.54364, nmi = 0.00392, ari = 0.00738  ; loss= 0.00752\n",
      "Iter 15680: acc = 0.54342, nmi = 0.00389, ari = 0.00731  ; loss= 0.00831\n",
      "Iter 15820: acc = 0.53824, nmi = 0.00291, ari = 0.00563  ; loss= 0.00987\n",
      "Iter 15960: acc = 0.53907, nmi = 0.00290, ari = 0.00583  ; loss= 0.00783\n",
      "Iter 16100: acc = 0.53769, nmi = 0.00261, ari = 0.00539  ; loss= 0.01204\n",
      "Iter 16240: acc = 0.53647, nmi = 0.00249, ari = 0.00507  ; loss= 0.01025\n",
      "Iter 16380: acc = 0.53452, nmi = 0.00224, ari = 0.00454  ; loss= 0.00771\n",
      "Iter 16520: acc = 0.53519, nmi = 0.00246, ari = 0.00477  ; loss= 0.00926\n",
      "Iter 16660: acc = 0.53634, nmi = 0.00277, ari = 0.00513  ; loss= 0.00885\n",
      "Iter 16800: acc = 0.54020, nmi = 0.00409, ari = 0.00641  ; loss= 0.01955\n",
      "Iter 16940: acc = 0.53987, nmi = 0.00425, ari = 0.00633  ; loss= 0.0168\n",
      "Iter 17080: acc = 0.53889, nmi = 0.00389, ari = 0.00600  ; loss= 0.01194\n",
      "Iter 17220: acc = 0.54050, nmi = 0.00396, ari = 0.00648  ; loss= 0.01544\n",
      "Iter 17360: acc = 0.53974, nmi = 0.00379, ari = 0.00624  ; loss= 0.00731\n",
      "Iter 17500: acc = 0.53932, nmi = 0.00355, ari = 0.00608  ; loss= 0.09274\n",
      "Iter 17640: acc = 0.53762, nmi = 0.00330, ari = 0.00557  ; loss= 0.00851\n",
      "Iter 17780: acc = 0.53954, nmi = 0.00356, ari = 0.00614  ; loss= 0.00887\n",
      "Iter 17920: acc = 0.53990, nmi = 0.00373, ari = 0.00627  ; loss= 0.01141\n",
      "Iter 18060: acc = 0.54191, nmi = 0.00422, ari = 0.00694  ; loss= 0.01299\n",
      "Iter 18200: acc = 0.54212, nmi = 0.00434, ari = 0.00702  ; loss= 0.00994\n",
      "Iter 18340: acc = 0.54117, nmi = 0.00407, ari = 0.00669  ; loss= 0.00683\n",
      "Iter 18480: acc = 0.54537, nmi = 0.00575, ari = 0.00821  ; loss= 0.0132\n",
      "Iter 18620: acc = 0.54684, nmi = 0.00601, ari = 0.00875  ; loss= 0.01877\n",
      "saving model to: results/complete_link_agglomerative_clustering/DEC_model_18740.h5\n",
      "Iter 18760: acc = 0.54816, nmi = 0.00615, ari = 0.00923  ; loss= 0.0067\n",
      "Iter 18900: acc = 0.54631, nmi = 0.00567, ari = 0.00853  ; loss= 0.01223\n",
      "Iter 19040: acc = 0.54868, nmi = 0.00601, ari = 0.00941  ; loss= 0.01465\n",
      "Iter 19180: acc = 0.54913, nmi = 0.00605, ari = 0.00957  ; loss= 0.00817\n",
      "Iter 19320: acc = 0.55093, nmi = 0.00703, ari = 0.01034  ; loss= 0.00753\n",
      "Iter 19460: acc = 0.55063, nmi = 0.00752, ari = 0.01024  ; loss= 0.14323\n",
      "Iter 19600: acc = 0.55104, nmi = 0.00771, ari = 0.01041  ; loss= 0.00714\n",
      "Iter 19740: acc = 0.55054, nmi = 0.00785, ari = 0.01020  ; loss= 0.02652\n",
      "Iter 19880: acc = 0.54761, nmi = 0.00812, ari = 0.00897  ; loss= 0.07154\n",
      "saving model to: results/complete_link_agglomerative_clustering/DEC_model_final.h5\n",
      "acc: 0.5476087246531484\n",
      "CPU times: user 12min 56s, sys: 1min 44s, total: 14min 41s\n",
      "Wall time: 11min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dec_model.compile(optimizer=SGD(0.01, 0.9), loss='kld')\n",
    "y_pred = dec_model.fit(train_features, y=train_labels, batch_size=32, save_dir=results_save_dir, \n",
    "                       cluster_linkage='complete')\n",
    "print('acc:', metrics.acc(train_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the features extracted by the DEC model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Output Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dec_encoder_model_pred = dec_model.encoder.predict(train_features)\n",
    "pca_mod = PCA(2)\n",
    "pca_mod.fit(dec_encoder_model_pred)\n",
    "reduced_features = pca_mod.transform(dec_encoder_model_pred)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.scatterplot(reduced_features[:, 0], reduced_features[:, 1], \n",
    "                hue=np.where(train_labels==0, 'elliptical', 'spiral'), ax=ax)\n",
    "ax.text(0.02, 0.92, f'Explained Var: {np.round(np.sum(pca_mod.explained_variance_ratio_), decimals=4)}', \n",
    "        transform=ax.transAxes)\n",
    "ax.set_xlabel('PCA Dim 1')\n",
    "ax.set_ylabel('PCA Dim 2')\n",
    "ax.set_title('PCA Scatterplot for Encoder Features')\n",
    "ax.legend(loc=4)\n",
    "plt.savefig(f'{results_save_dir}/PCAencoding_pca_features.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_log, ptrain_log = pd.read_csv(f'{results_save_dir}/dec_log.csv'), pd.read_csv(f'{results_save_dir}/pretrain_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7105afd08158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mptrain_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Reconstruction Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdec_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Clustering Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdec_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nmi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Clustering NMI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{results_save_dir}/training_curves.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "ptrain_log.plot(x='epoch', y='loss', ax=ax[0], title='Reconstruction Loss')\n",
    "dec_log.plot(x='iter', y='acc', ax=ax[1], title='Clustering Accuracy')\n",
    "dec_log.plot(x='iter', y='nmi', ax=ax[-1], title='Clustering NMI')\n",
    "plt.savefig(f'{results_save_dir}/training_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average-Link Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEC Xception Training Regime\n",
    "\n",
    "This part of the notebook defines the generator for the training regime of the DEC model over the features extracted from the Xception architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 500)               1024500   \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "encoder_2 (Dense)            (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "encoder_3 (Dense)            (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 2)                 20        \n",
      "=================================================================\n",
      "Total params: 2,297,030\n",
      "Trainable params: 2,297,030\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defining our DEC model\n",
    "dec_model = DEC_Agglomerative([2048, 500, 500, 2000, 10], n_clusters=2)\n",
    "dec_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_save_dir = 'results/average_link_agglomerative_clustering'\n",
    "if not exist_directory(results_save_dir):\n",
    "    os.makedirs(results_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Pretraining...\n",
      "Epoch 1/100\n",
      "59968/59968 [==============================] - 5s 83us/step - loss: 0.0149\n",
      "        |==>  acc: 0.6642,  nmi: 0.0724  <==|\n",
      "Epoch 2/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0079\n",
      "Epoch 3/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0070\n",
      "Epoch 4/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0067\n",
      "Epoch 5/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0065\n",
      "Epoch 6/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0064\n",
      "Epoch 7/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0063\n",
      "Epoch 8/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0062\n",
      "Epoch 9/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0061\n",
      "Epoch 10/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0060\n",
      "Epoch 11/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0060\n",
      "        |==>  acc: 0.6798,  nmi: 0.1148  <==|\n",
      "Epoch 12/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0059\n",
      "Epoch 13/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0059\n",
      "Epoch 14/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0058\n",
      "Epoch 15/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0058\n",
      "Epoch 16/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0058\n",
      "Epoch 17/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0057\n",
      "Epoch 18/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0057\n",
      "Epoch 19/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0057\n",
      "Epoch 20/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0057\n",
      "Epoch 21/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0056\n",
      "        |==>  acc: 0.7201,  nmi: 0.1358  <==|\n",
      "Epoch 22/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0056\n",
      "Epoch 23/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0056\n",
      "Epoch 24/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0056\n",
      "Epoch 25/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0055\n",
      "Epoch 26/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0055\n",
      "Epoch 27/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0055\n",
      "Epoch 28/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0055\n",
      "Epoch 29/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0055\n",
      "Epoch 30/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0055\n",
      "Epoch 31/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0054\n",
      "        |==>  acc: 0.6778,  nmi: 0.0923  <==|\n",
      "Epoch 32/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0054\n",
      "Epoch 33/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0054\n",
      "Epoch 34/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0054\n",
      "Epoch 35/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0054\n",
      "Epoch 36/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0054\n",
      "Epoch 37/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0053\n",
      "Epoch 38/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0053\n",
      "Epoch 39/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0053\n",
      "Epoch 40/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0053\n",
      "Epoch 41/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0053\n",
      "        |==>  acc: 0.7098,  nmi: 0.1226  <==|\n",
      "Epoch 42/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0053\n",
      "Epoch 43/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0053\n",
      "Epoch 44/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "Epoch 45/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "Epoch 46/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "Epoch 47/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "Epoch 48/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "Epoch 49/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "Epoch 50/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "Epoch 51/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "        |==>  acc: 0.7189,  nmi: 0.1507  <==|\n",
      "Epoch 52/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0052\n",
      "Epoch 53/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0051\n",
      "Epoch 54/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0051\n",
      "Epoch 55/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0051\n",
      "Epoch 56/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0051\n",
      "Epoch 57/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0051\n",
      "Epoch 58/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0051\n",
      "Epoch 59/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 60/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 61/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0051\n",
      "        |==>  acc: 0.6784,  nmi: 0.0899  <==|\n",
      "Epoch 62/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0051\n",
      "Epoch 63/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0051\n",
      "Epoch 64/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0051\n",
      "Epoch 65/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 66/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 67/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 68/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 69/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 70/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 71/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "        |==>  acc: 0.6699,  nmi: 0.0788  <==|\n",
      "Epoch 72/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0050\n",
      "Epoch 73/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0050\n",
      "Epoch 74/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0050\n",
      "Epoch 75/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0049\n",
      "Epoch 76/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0049\n",
      "Epoch 77/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0049\n",
      "Epoch 78/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0049\n",
      "Epoch 79/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0049\n",
      "Epoch 80/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0049\n",
      "Epoch 81/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "        |==>  acc: 0.7041,  nmi: 0.1227  <==|\n",
      "Epoch 82/100\n",
      "59968/59968 [==============================] - 4s 73us/step - loss: 0.0049\n",
      "Epoch 83/100\n",
      "59968/59968 [==============================] - 4s 71us/step - loss: 0.0049\n",
      "Epoch 84/100\n",
      "59968/59968 [==============================] - 4s 70us/step - loss: 0.0049\n",
      "Epoch 85/100\n",
      "59968/59968 [==============================] - 4s 71us/step - loss: 0.0049\n",
      "Epoch 86/100\n",
      "59968/59968 [==============================] - 4s 70us/step - loss: 0.0049\n",
      "Epoch 87/100\n",
      "59968/59968 [==============================] - 4s 70us/step - loss: 0.0049\n",
      "Epoch 88/100\n",
      "59968/59968 [==============================] - 4s 70us/step - loss: 0.0049\n",
      "Epoch 89/100\n",
      "59968/59968 [==============================] - 4s 69us/step - loss: 0.0048\n",
      "Epoch 90/100\n",
      "59968/59968 [==============================] - 4s 71us/step - loss: 0.0048\n",
      "Epoch 91/100\n",
      "59968/59968 [==============================] - 4s 69us/step - loss: 0.0048\n",
      "        |==>  acc: 0.6228,  nmi: 0.0613  <==|\n",
      "Epoch 92/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0048\n",
      "Epoch 93/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 94/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 95/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0048\n",
      "Epoch 96/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0048\n",
      "Epoch 97/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0048\n",
      "Epoch 98/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0048\n",
      "Epoch 99/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0048\n",
      "Epoch 100/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0048\n",
      "Pretraining time: 1707s\n",
      "Pretrained weights are saved to results/average_link_agglomerative_clustering/ae_weights.h5\n",
      "CPU times: user 26min 50s, sys: 2min 27s, total: 29min 17s\n",
      "Wall time: 28min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dec_model.pretrain(train_features, train_labels, epochs=100, save_dir=results_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 140\n",
      "Save interval 9370\n",
      "Initializing cluster centers with k-means.\n",
      "Iter 0: acc = 0.56735, nmi = 0.00490, ari = -0.00059  ; loss= 0\n",
      "saving model to: results/average_link_agglomerative_clustering/DEC_model_0.h5\n",
      "Iter 140: acc = 0.55823, nmi = 0.00746, ari = -0.00398  ; loss= 0.00264\n",
      "Iter 280: acc = 0.54219, nmi = 0.00513, ari = -0.00679  ; loss= 0.00489\n",
      "Iter 420: acc = 0.52862, nmi = 0.00507, ari = -0.00750  ; loss= 0.00989\n",
      "Iter 560: acc = 0.51809, nmi = 0.00506, ari = -0.00709  ; loss= 0.03567\n",
      "Iter 700: acc = 0.51259, nmi = 0.00518, ari = -0.00663  ; loss= 0.023\n",
      "Iter 840: acc = 0.50929, nmi = 0.00546, ari = -0.00639  ; loss= 0.02766\n",
      "Iter 980: acc = 0.50439, nmi = 0.00533, ari = -0.00559  ; loss= 0.05472\n",
      "Iter 1120: acc = 0.50190, nmi = 0.00490, ari = -0.00489  ; loss= 0.03293\n",
      "Iter 1260: acc = 0.50419, nmi = 0.00534, ari = -0.00398  ; loss= 0.05221\n",
      "Iter 1400: acc = 0.50734, nmi = 0.00510, ari = -0.00314  ; loss= 0.03244\n",
      "Iter 1540: acc = 0.50754, nmi = 0.00397, ari = -0.00238  ; loss= 0.03446\n",
      "Iter 1680: acc = 0.50714, nmi = 0.00315, ari = -0.00191  ; loss= 0.05644\n",
      "Iter 1820: acc = 0.50792, nmi = 0.00318, ari = -0.00177  ; loss= 0.05696\n",
      "Iter 1960: acc = 0.50732, nmi = 0.00324, ari = -0.00193  ; loss= 0.03673\n",
      "Iter 2100: acc = 0.50927, nmi = 0.00305, ari = -0.00139  ; loss= 0.0353\n",
      "Iter 2240: acc = 0.50999, nmi = 0.00236, ari = -0.00077  ; loss= 0.04376\n",
      "Iter 2380: acc = 0.51196, nmi = 0.00310, ari = -0.00084  ; loss= 0.04895\n",
      "Iter 2520: acc = 0.51157, nmi = 0.00271, ari = -0.00068  ; loss= 0.04421\n",
      "Iter 2660: acc = 0.51236, nmi = 0.00291, ari = -0.00063  ; loss= 0.03773\n",
      "Iter 2800: acc = 0.51231, nmi = 0.00266, ari = -0.00049  ; loss= 0.03589\n",
      "Iter 2940: acc = 0.51392, nmi = 0.00276, ari = -0.00020  ; loss= 0.02482\n",
      "Iter 3080: acc = 0.51681, nmi = 0.00287, ari = 0.00039  ; loss= 0.02362\n",
      "Iter 3220: acc = 0.51476, nmi = 0.00244, ari = 0.00016  ; loss= 0.02483\n",
      "Iter 3360: acc = 0.51324, nmi = 0.00220, ari = -0.00002  ; loss= 0.02641\n",
      "Iter 3500: acc = 0.51231, nmi = 0.00194, ari = -0.00005  ; loss= 0.0254\n",
      "Iter 3640: acc = 0.50744, nmi = 0.00066, ari = -0.00001  ; loss= 0.11987\n",
      "Iter 3780: acc = 0.50774, nmi = 0.00053, ari = 0.00010  ; loss= 0.19561\n",
      "Iter 3920: acc = 0.50604, nmi = 0.00056, ari = -0.00010  ; loss= 0.01959\n",
      "Iter 4060: acc = 0.50492, nmi = 0.00025, ari = 0.00001  ; loss= 0.02892\n",
      "Iter 4200: acc = 0.50095, nmi = 0.00007, ari = -0.00014  ; loss= 0.01853\n",
      "Iter 4340: acc = 0.51076, nmi = 0.00001, ari = 0.00010  ; loss= 0.08456\n",
      "Iter 4480: acc = 0.50999, nmi = 0.00001, ari = -0.00019  ; loss= 0.02204\n",
      "Iter 4620: acc = 0.50625, nmi = 0.00005, ari = -0.00028  ; loss= 0.02465\n",
      "Iter 4760: acc = 0.50564, nmi = 0.00000, ari = -0.00000  ; loss= 0.04236\n",
      "Iter 4900: acc = 0.50504, nmi = 0.00001, ari = 0.00004  ; loss= 0.03131\n",
      "Iter 5040: acc = 0.50555, nmi = 0.00001, ari = 0.00005  ; loss= 0.02055\n",
      "Iter 5180: acc = 0.50319, nmi = 0.00000, ari = -0.00002  ; loss= 0.02618\n",
      "Iter 5320: acc = 0.50080, nmi = 0.00002, ari = -0.00006  ; loss= 0.03845\n",
      "Iter 5460: acc = 0.50233, nmi = 0.00003, ari = -0.00012  ; loss= 0.01668\n",
      "Iter 5600: acc = 0.50380, nmi = 0.00003, ari = -0.00014  ; loss= 0.02874\n",
      "Iter 5740: acc = 0.50489, nmi = 0.00002, ari = -0.00014  ; loss= 0.01374\n",
      "Iter 5880: acc = 0.50469, nmi = 0.00001, ari = -0.00011  ; loss= 0.01667\n",
      "Iter 6020: acc = 0.50859, nmi = 0.00004, ari = 0.00018  ; loss= 0.02812\n",
      "Iter 6160: acc = 0.50927, nmi = 0.00006, ari = 0.00024  ; loss= 0.02301\n",
      "Iter 6300: acc = 0.50605, nmi = 0.00003, ari = 0.00009  ; loss= 0.02472\n",
      "Iter 6440: acc = 0.50760, nmi = 0.00004, ari = 0.00016  ; loss= 0.03347\n",
      "Iter 6580: acc = 0.50885, nmi = 0.00001, ari = 0.00009  ; loss= 0.05453\n",
      "Iter 6720: acc = 0.50734, nmi = 0.00000, ari = 0.00004  ; loss= 0.0169\n",
      "Iter 6860: acc = 0.50967, nmi = 0.00004, ari = 0.00022  ; loss= 0.01204\n",
      "Iter 7000: acc = 0.51031, nmi = 0.00003, ari = 0.00021  ; loss= 0.02644\n",
      "Iter 7140: acc = 0.50949, nmi = 0.00002, ari = 0.00016  ; loss= 0.02304\n",
      "Iter 7280: acc = 0.50812, nmi = 0.00000, ari = 0.00002  ; loss= 0.0127\n",
      "Iter 7420: acc = 0.50777, nmi = 0.00002, ari = 0.00011  ; loss= 0.01306\n",
      "Iter 7560: acc = 0.50981, nmi = 0.00003, ari = 0.00019  ; loss= 0.0124\n",
      "Iter 7700: acc = 0.50874, nmi = 0.00000, ari = 0.00002  ; loss= 0.01534\n",
      "Iter 7840: acc = 0.51729, nmi = 0.00020, ari = 0.00084  ; loss= 0.0108\n",
      "Iter 7980: acc = 0.51386, nmi = 0.00000, ari = 0.00008  ; loss= 0.21024\n",
      "Iter 8120: acc = 0.51758, nmi = 0.00004, ari = 0.00042  ; loss= 0.04558\n",
      "Iter 8260: acc = 0.51661, nmi = 0.00002, ari = 0.00028  ; loss= 0.01297\n",
      "Iter 8400: acc = 0.51389, nmi = 0.00003, ari = 0.00027  ; loss= 0.01135\n",
      "Iter 8540: acc = 0.51544, nmi = 0.00007, ari = 0.00050  ; loss= 0.0226\n",
      "Iter 8680: acc = 0.51294, nmi = 0.00005, ari = 0.00035  ; loss= 0.04337\n",
      "Iter 8820: acc = 0.51209, nmi = 0.00007, ari = 0.00035  ; loss= 0.02045\n",
      "Iter 8960: acc = 0.51016, nmi = 0.00002, ari = 0.00016  ; loss= 0.02668\n",
      "Iter 9100: acc = 0.50875, nmi = 0.00000, ari = 0.00000  ; loss= 0.01173\n",
      "Iter 9240: acc = 0.50830, nmi = 0.00000, ari = -0.00010  ; loss= 0.01365\n",
      "saving model to: results/average_link_agglomerative_clustering/DEC_model_9370.h5\n",
      "Iter 9380: acc = 0.50657, nmi = 0.00002, ari = -0.00017  ; loss= 0.01332\n",
      "Iter 9520: acc = 0.50600, nmi = 0.00005, ari = -0.00029  ; loss= 0.01218\n",
      "Iter 9660: acc = 0.50632, nmi = 0.00005, ari = -0.00029  ; loss= 0.01666\n",
      "Iter 9800: acc = 0.50887, nmi = 0.00001, ari = -0.00014  ; loss= 0.01151\n",
      "Iter 9940: acc = 0.50775, nmi = 0.00002, ari = -0.00020  ; loss= 0.02015\n",
      "Iter 10080: acc = 0.50842, nmi = 0.00003, ari = -0.00027  ; loss= 0.00994\n",
      "Iter 10220: acc = 0.50003, nmi = 0.00026, ari = -0.00037  ; loss= 0.01696\n",
      "Iter 10360: acc = 0.50183, nmi = 0.00030, ari = -0.00054  ; loss= 0.02605\n",
      "Iter 10500: acc = 0.50474, nmi = 0.00014, ari = -0.00046  ; loss= 0.01182\n",
      "Iter 10640: acc = 0.50450, nmi = 0.00015, ari = -0.00047  ; loss= 0.01132\n",
      "Iter 10780: acc = 0.50245, nmi = 0.00016, ari = -0.00037  ; loss= 0.01167\n",
      "Iter 10920: acc = 0.50058, nmi = 0.00024, ari = -0.00038  ; loss= 0.00968\n",
      "Iter 11060: acc = 0.50012, nmi = 0.00016, ari = -0.00023  ; loss= 0.00826\n",
      "Iter 11200: acc = 0.50063, nmi = 0.00011, ari = -0.00019  ; loss= 0.02088\n",
      "Iter 11340: acc = 0.50302, nmi = 0.00001, ari = -0.00007  ; loss= 0.05492\n",
      "Iter 11480: acc = 0.50250, nmi = 0.00000, ari = -0.00001  ; loss= 0.04332\n",
      "Iter 11620: acc = 0.50155, nmi = 0.00000, ari = -0.00003  ; loss= 0.01045\n",
      "Iter 11760: acc = 0.50272, nmi = 0.00000, ari = -0.00001  ; loss= 0.00968\n",
      "Iter 11900: acc = 0.50203, nmi = 0.00005, ari = -0.00002  ; loss= 0.00921\n",
      "Iter 12040: acc = 0.50125, nmi = 0.00001, ari = -0.00001  ; loss= 0.0103\n",
      "Iter 12180: acc = 0.50228, nmi = 0.00003, ari = 0.00000  ; loss= 0.05476\n",
      "Iter 12320: acc = 0.50699, nmi = 0.00008, ari = 0.00017  ; loss= 0.04957\n",
      "Iter 12460: acc = 0.50600, nmi = 0.00008, ari = 0.00013  ; loss= 0.01004\n",
      "Iter 12600: acc = 0.50652, nmi = 0.00005, ari = 0.00013  ; loss= 0.01021\n",
      "Iter 12740: acc = 0.50494, nmi = 0.00004, ari = 0.00007  ; loss= 0.01657\n",
      "Iter 12880: acc = 0.50592, nmi = 0.00005, ari = 0.00011  ; loss= 0.02435\n",
      "Iter 13020: acc = 0.50675, nmi = 0.00011, ari = 0.00016  ; loss= 0.02099\n",
      "Iter 13160: acc = 0.50504, nmi = 0.00002, ari = 0.00006  ; loss= 0.00922\n",
      "Iter 13300: acc = 0.50160, nmi = 0.00002, ari = -0.00008  ; loss= 0.01234\n",
      "Iter 13440: acc = 0.50123, nmi = 0.00002, ari = -0.00006  ; loss= 0.04653\n",
      "Iter 13580: acc = 0.50172, nmi = 0.00003, ari = -0.00009  ; loss= 0.01308\n",
      "Iter 13720: acc = 0.50285, nmi = 0.00003, ari = -0.00013  ; loss= 0.01181\n",
      "Iter 13860: acc = 0.50153, nmi = 0.00004, ari = -0.00011  ; loss= 0.00854\n",
      "Iter 14000: acc = 0.50078, nmi = 0.00007, ari = -0.00008  ; loss= 0.00854\n",
      "Iter 14140: acc = 0.50082, nmi = 0.00005, ari = -0.00006  ; loss= 0.01072\n",
      "Iter 14280: acc = 0.50050, nmi = 0.00004, ari = -0.00005  ; loss= 0.00874\n",
      "Iter 14420: acc = 0.50479, nmi = 0.00012, ari = 0.00007  ; loss= 0.01513\n",
      "Iter 14560: acc = 0.50815, nmi = 0.00008, ari = 0.00022  ; loss= 0.10335\n",
      "Iter 14700: acc = 0.50805, nmi = 0.00010, ari = 0.00022  ; loss= 0.01053\n",
      "Iter 14840: acc = 0.51094, nmi = 0.00024, ari = 0.00045  ; loss= 0.03179\n",
      "Iter 14980: acc = 0.51007, nmi = 0.00041, ari = 0.00038  ; loss= 0.02228\n",
      "Iter 15120: acc = 0.51086, nmi = 0.00040, ari = 0.00045  ; loss= 0.00667\n",
      "Iter 15260: acc = 0.50941, nmi = 0.00035, ari = 0.00033  ; loss= 0.02727\n",
      "Iter 15400: acc = 0.51086, nmi = 0.00045, ari = 0.00045  ; loss= 0.04254\n",
      "Iter 15540: acc = 0.51201, nmi = 0.00045, ari = 0.00056  ; loss= 0.03607\n",
      "Iter 15680: acc = 0.51566, nmi = 0.00088, ari = 0.00095  ; loss= 0.01609\n",
      "Iter 15820: acc = 0.51562, nmi = 0.00102, ari = 0.00093  ; loss= 0.00999\n",
      "Iter 15960: acc = 0.51594, nmi = 0.00090, ari = 0.00099  ; loss= 0.0093\n",
      "Iter 16100: acc = 0.51654, nmi = 0.00106, ari = 0.00106  ; loss= 0.01263\n",
      "Iter 16240: acc = 0.51778, nmi = 0.00125, ari = 0.00122  ; loss= 0.01012\n",
      "Iter 16380: acc = 0.51346, nmi = 0.00078, ari = 0.00068  ; loss= 0.04008\n",
      "Iter 16520: acc = 0.51061, nmi = 0.00043, ari = 0.00043  ; loss= 0.02365\n",
      "Iter 16660: acc = 0.51336, nmi = 0.00072, ari = 0.00068  ; loss= 0.00964\n",
      "Iter 16800: acc = 0.51407, nmi = 0.00062, ari = 0.00078  ; loss= 0.00692\n",
      "Iter 16940: acc = 0.51237, nmi = 0.00056, ari = 0.00059  ; loss= 0.02628\n",
      "Iter 17080: acc = 0.51136, nmi = 0.00042, ari = 0.00050  ; loss= 0.03934\n",
      "Iter 17220: acc = 0.51277, nmi = 0.00033, ari = 0.00062  ; loss= 0.01214\n",
      "Iter 17360: acc = 0.51229, nmi = 0.00028, ari = 0.00056  ; loss= 0.00973\n",
      "Iter 17500: acc = 0.51026, nmi = 0.00020, ari = 0.00039  ; loss= 0.01123\n",
      "Iter 17640: acc = 0.51072, nmi = 0.00028, ari = 0.00044  ; loss= 0.00698\n",
      "Iter 17780: acc = 0.51166, nmi = 0.00035, ari = 0.00052  ; loss= 0.01528\n",
      "Iter 17920: acc = 0.51082, nmi = 0.00022, ari = 0.00043  ; loss= 0.01881\n",
      "Iter 18060: acc = 0.51496, nmi = 0.00052, ari = 0.00087  ; loss= 0.01342\n",
      "Iter 18200: acc = 0.51476, nmi = 0.00045, ari = 0.00083  ; loss= 0.01192\n",
      "Iter 18340: acc = 0.51399, nmi = 0.00037, ari = 0.00073  ; loss= 0.00711\n",
      "Iter 18480: acc = 0.51256, nmi = 0.00033, ari = 0.00060  ; loss= 0.04254\n",
      "Iter 18620: acc = 0.50714, nmi = 0.00005, ari = 0.00015  ; loss= 0.02348\n",
      "saving model to: results/average_link_agglomerative_clustering/DEC_model_18740.h5\n",
      "Iter 18760: acc = 0.50685, nmi = 0.00006, ari = 0.00015  ; loss= 0.00652\n",
      "Iter 18900: acc = 0.50754, nmi = 0.00008, ari = 0.00019  ; loss= 0.00748\n",
      "Iter 19040: acc = 0.50804, nmi = 0.00011, ari = 0.00023  ; loss= 0.00666\n",
      "Iter 19180: acc = 0.50515, nmi = 0.00001, ari = 0.00004  ; loss= 0.00999\n",
      "Iter 19320: acc = 0.50664, nmi = 0.00005, ari = 0.00014  ; loss= 0.00686\n",
      "Iter 19460: acc = 0.50477, nmi = 0.00004, ari = 0.00007  ; loss= 0.0074\n",
      "Iter 19600: acc = 0.50764, nmi = 0.00007, ari = 0.00019  ; loss= 0.02607\n",
      "Iter 19740: acc = 0.50772, nmi = 0.00010, ari = 0.00021  ; loss= 0.03119\n",
      "Iter 19880: acc = 0.50332, nmi = 0.00002, ari = 0.00002  ; loss= 0.01455\n",
      "saving model to: results/average_link_agglomerative_clustering/DEC_model_final.h5\n",
      "acc: 0.5033184364994664\n",
      "CPU times: user 13min 17s, sys: 1min 45s, total: 15min 2s\n",
      "Wall time: 11min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dec_model.compile(optimizer=SGD(0.01, 0.9), loss='kld')\n",
    "y_pred = dec_model.fit(train_features, y=train_labels, batch_size=32, save_dir=results_save_dir, \n",
    "                       cluster_linkage='average')\n",
    "print('acc:', metrics.acc(train_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the features extracted by the DEC model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Output Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dec_encoder_model_pred = dec_model.encoder.predict(train_features)\n",
    "pca_mod = PCA(2)\n",
    "pca_mod.fit(dec_encoder_model_pred)\n",
    "reduced_features = pca_mod.transform(dec_encoder_model_pred)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.scatterplot(reduced_features[:, 0], reduced_features[:, 1], \n",
    "                hue=np.where(train_labels==0, 'elliptical', 'spiral'), ax=ax)\n",
    "ax.text(0.02, 0.92, f'Explained Var: {np.round(np.sum(pca_mod.explained_variance_ratio_), decimals=4)}', \n",
    "        transform=ax.transAxes)\n",
    "ax.set_xlabel('PCA Dim 1')\n",
    "ax.set_ylabel('PCA Dim 2')\n",
    "ax.set_title('PCA Scatterplot for Encoder Features')\n",
    "ax.legend(loc=4)\n",
    "plt.savefig(f'{results_save_dir}/PCAencoding_pca_features.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_log, ptrain_log = pd.read_csv(f'{results_save_dir}/dec_log.csv'), pd.read_csv(f'{results_save_dir}/pretrain_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-7105afd08158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mptrain_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Reconstruction Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdec_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Clustering Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdec_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nmi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Clustering NMI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{results_save_dir}/training_curves.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "ptrain_log.plot(x='epoch', y='loss', ax=ax[0], title='Reconstruction Loss')\n",
    "dec_log.plot(x='iter', y='acc', ax=ax[1], title='Clustering Accuracy')\n",
    "dec_log.plot(x='iter', y='nmi', ax=ax[-1], title='Clustering NMI')\n",
    "plt.savefig(f'{results_save_dir}/training_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ward-Link Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEC Xception Training Regime\n",
    "\n",
    "This part of the notebook defines the generator for the training regime of the DEC model over the features extracted from the Xception architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 500)               1024500   \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "encoder_2 (Dense)            (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "encoder_3 (Dense)            (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 2)                 20        \n",
      "=================================================================\n",
      "Total params: 2,297,030\n",
      "Trainable params: 2,297,030\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defining our DEC model\n",
    "dec_model = DEC_Agglomerative([2048, 500, 500, 2000, 10], n_clusters=2)\n",
    "dec_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_save_dir = 'results/ward_link_agglomerative_clustering'\n",
    "if not exist_directory(results_save_dir):\n",
    "    os.makedirs(results_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Pretraining...\n",
      "Epoch 1/100\n",
      "59968/59968 [==============================] - 5s 86us/step - loss: 0.0153\n",
      "        |==>  acc: 0.6569,  nmi: 0.0628  <==|\n",
      "Epoch 2/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0080\n",
      "Epoch 3/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0070\n",
      "Epoch 4/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0067\n",
      "Epoch 5/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0065\n",
      "Epoch 6/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0064\n",
      "Epoch 7/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0062\n",
      "Epoch 8/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0062\n",
      "Epoch 9/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0061\n",
      "Epoch 10/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0060\n",
      "Epoch 11/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0060\n",
      "        |==>  acc: 0.7140,  nmi: 0.1303  <==|\n",
      "Epoch 12/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0059\n",
      "Epoch 13/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0059\n",
      "Epoch 14/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0058\n",
      "Epoch 15/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0058\n",
      "Epoch 16/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0058\n",
      "Epoch 17/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0058\n",
      "Epoch 18/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0057\n",
      "Epoch 19/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0057\n",
      "Epoch 20/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0056\n",
      "Epoch 21/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0056\n",
      "        |==>  acc: 0.6908,  nmi: 0.0985  <==|\n",
      "Epoch 22/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0056\n",
      "Epoch 23/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0056\n",
      "Epoch 24/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0056\n",
      "Epoch 25/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0055\n",
      "Epoch 26/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0055\n",
      "Epoch 27/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0055\n",
      "Epoch 28/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0055\n",
      "Epoch 29/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0055\n",
      "Epoch 30/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0055\n",
      "Epoch 31/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0054\n",
      "        |==>  acc: 0.6216,  nmi: 0.0383  <==|\n",
      "Epoch 32/100\n",
      "59968/59968 [==============================] - 4s 69us/step - loss: 0.0054\n",
      "Epoch 33/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0054\n",
      "Epoch 34/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0054\n",
      "Epoch 35/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0054\n",
      "Epoch 36/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0054\n",
      "Epoch 37/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0054\n",
      "Epoch 38/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0053\n",
      "Epoch 39/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0053\n",
      "Epoch 40/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0053\n",
      "Epoch 41/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0053\n",
      "        |==>  acc: 0.6337,  nmi: 0.0453  <==|\n",
      "Epoch 42/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0053\n",
      "Epoch 43/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0053\n",
      "Epoch 44/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0053\n",
      "Epoch 45/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "Epoch 46/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "Epoch 47/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "Epoch 48/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "Epoch 49/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "Epoch 50/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "Epoch 51/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0052\n",
      "        |==>  acc: 0.6695,  nmi: 0.0806  <==|\n",
      "Epoch 52/100\n",
      "59968/59968 [==============================] - 4s 68us/step - loss: 0.0052\n",
      "Epoch 53/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0052\n",
      "Epoch 54/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 55/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0051\n",
      "Epoch 56/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 57/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 58/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0051\n",
      "Epoch 59/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 60/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "Epoch 61/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0051\n",
      "        |==>  acc: 0.6552,  nmi: 0.0611  <==|\n",
      "Epoch 62/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0050\n",
      "Epoch 63/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 64/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 65/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 66/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 67/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0050\n",
      "Epoch 68/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 69/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 70/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 71/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0050\n",
      "        |==>  acc: 0.6835,  nmi: 0.0901  <==|\n",
      "Epoch 72/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0050\n",
      "Epoch 73/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 74/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 75/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 76/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0050\n",
      "Epoch 77/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 78/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 79/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 80/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 81/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "        |==>  acc: 0.7217,  nmi: 0.1423  <==|\n",
      "Epoch 82/100\n",
      "59968/59968 [==============================] - 4s 66us/step - loss: 0.0049\n",
      "Epoch 83/100\n",
      "59968/59968 [==============================] - 4s 64us/step - loss: 0.0049\n",
      "Epoch 84/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 85/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 86/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 87/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 88/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0049\n",
      "Epoch 89/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 90/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 91/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "        |==>  acc: 0.6307,  nmi: 0.0630  <==|\n",
      "Epoch 92/100\n",
      "59968/59968 [==============================] - 4s 67us/step - loss: 0.0048\n",
      "Epoch 93/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 94/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 95/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 96/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 97/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 98/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 99/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Epoch 100/100\n",
      "59968/59968 [==============================] - 4s 65us/step - loss: 0.0048\n",
      "Pretraining time: 1703s\n",
      "Pretrained weights are saved to results/ward_link_agglomerative_clustering/ae_weights.h5\n",
      "CPU times: user 26min 45s, sys: 2min 27s, total: 29min 13s\n",
      "Wall time: 28min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dec_model.pretrain(train_features, train_labels, epochs=100, save_dir=results_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 140\n",
      "Save interval 9370\n",
      "Initializing cluster centers with k-means.\n",
      "Iter 0: acc = 0.55953, nmi = 0.00361, ari = -0.00314  ; loss= 0\n",
      "saving model to: results/ward_link_agglomerative_clustering/DEC_model_0.h5\n",
      "Iter 140: acc = 0.54229, nmi = 0.00183, ari = -0.00487  ; loss= 0.00211\n",
      "Iter 280: acc = 0.52716, nmi = 0.00138, ari = -0.00434  ; loss= 0.00554\n",
      "Iter 420: acc = 0.52031, nmi = 0.00089, ari = -0.00315  ; loss= 0.01552\n",
      "Iter 560: acc = 0.52019, nmi = 0.00051, ari = -0.00233  ; loss= 0.04956\n",
      "Iter 700: acc = 0.52021, nmi = 0.00027, ari = -0.00167  ; loss= 0.02963\n",
      "Iter 840: acc = 0.51893, nmi = 0.00032, ari = -0.00175  ; loss= 0.03638\n",
      "Iter 980: acc = 0.51614, nmi = 0.00028, ari = -0.00146  ; loss= 0.05082\n",
      "Iter 1120: acc = 0.51928, nmi = 0.00020, ari = -0.00136  ; loss= 0.03777\n",
      "Iter 1260: acc = 0.51974, nmi = 0.00008, ari = -0.00088  ; loss= 0.0519\n",
      "Iter 1400: acc = 0.51963, nmi = 0.00001, ari = -0.00035  ; loss= 0.04321\n",
      "Iter 1540: acc = 0.52149, nmi = 0.00002, ari = 0.00040  ; loss= 0.04624\n",
      "Iter 1680: acc = 0.51946, nmi = 0.00000, ari = 0.00003  ; loss= 0.05311\n",
      "Iter 1820: acc = 0.51699, nmi = 0.00002, ari = -0.00035  ; loss= 0.05012\n",
      "Iter 1960: acc = 0.51472, nmi = 0.00015, ari = -0.00099  ; loss= 0.03547\n",
      "Iter 2100: acc = 0.51144, nmi = 0.00019, ari = -0.00095  ; loss= 0.02969\n",
      "Iter 2240: acc = 0.51783, nmi = 0.00001, ari = -0.00024  ; loss= 0.04219\n",
      "Iter 2380: acc = 0.51376, nmi = 0.00011, ari = -0.00078  ; loss= 0.04021\n",
      "Iter 2520: acc = 0.51337, nmi = 0.00017, ari = -0.00099  ; loss= 0.03476\n",
      "Iter 2660: acc = 0.51683, nmi = 0.00002, ari = -0.00041  ; loss= 0.03137\n",
      "Iter 2800: acc = 0.52086, nmi = 0.00000, ari = 0.00008  ; loss= 0.04514\n",
      "Iter 2940: acc = 0.51894, nmi = 0.00001, ari = -0.00029  ; loss= 0.03218\n",
      "Iter 3080: acc = 0.51804, nmi = 0.00010, ari = -0.00088  ; loss= 0.02203\n",
      "Iter 3220: acc = 0.51451, nmi = 0.00032, ari = -0.00148  ; loss= 0.06602\n",
      "Iter 3360: acc = 0.51509, nmi = 0.00043, ari = -0.00181  ; loss= 0.09705\n",
      "Iter 3500: acc = 0.51598, nmi = 0.00081, ari = -0.00265  ; loss= 0.06793\n",
      "Iter 3640: acc = 0.51457, nmi = 0.00116, ari = -0.00312  ; loss= 0.03045\n",
      "Iter 3780: acc = 0.50991, nmi = 0.00120, ari = -0.00270  ; loss= 0.02205\n",
      "Iter 3920: acc = 0.50884, nmi = 0.00106, ari = -0.00238  ; loss= 0.05567\n",
      "Iter 4060: acc = 0.50904, nmi = 0.00080, ari = -0.00201  ; loss= 0.04511\n",
      "Iter 4200: acc = 0.51067, nmi = 0.00038, ari = -0.00138  ; loss= 0.07839\n",
      "Iter 4340: acc = 0.50962, nmi = 0.00062, ari = -0.00176  ; loss= 0.02055\n",
      "Iter 4480: acc = 0.51182, nmi = 0.00051, ari = -0.00173  ; loss= 0.02648\n",
      "Iter 4620: acc = 0.51149, nmi = 0.00039, ari = -0.00144  ; loss= 0.02339\n",
      "Iter 4760: acc = 0.50904, nmi = 0.00012, ari = -0.00062  ; loss= 0.1759\n",
      "Iter 4900: acc = 0.50582, nmi = 0.00024, ari = -0.00072  ; loss= 0.01602\n",
      "Iter 5040: acc = 0.50860, nmi = 0.00000, ari = 0.00001  ; loss= 0.04631\n",
      "Iter 5180: acc = 0.50814, nmi = 0.00000, ari = -0.00000  ; loss= 0.0141\n",
      "Iter 5320: acc = 0.50877, nmi = 0.00001, ari = -0.00015  ; loss= 0.03167\n",
      "Iter 5460: acc = 0.50552, nmi = 0.00022, ari = -0.00066  ; loss= 0.07997\n",
      "Iter 5600: acc = 0.50872, nmi = 0.00005, ari = -0.00036  ; loss= 0.03048\n",
      "Iter 5740: acc = 0.50614, nmi = 0.00020, ari = -0.00065  ; loss= 0.01253\n",
      "Iter 5880: acc = 0.50540, nmi = 0.00011, ari = -0.00042  ; loss= 0.01811\n",
      "Iter 6020: acc = 0.50397, nmi = 0.00014, ari = -0.00042  ; loss= 0.0133\n",
      "Iter 6160: acc = 0.50865, nmi = 0.00001, ari = -0.00017  ; loss= 0.02986\n",
      "Iter 6300: acc = 0.50627, nmi = 0.00008, ari = -0.00036  ; loss= 0.01809\n",
      "Iter 6440: acc = 0.51007, nmi = 0.00009, ari = -0.00055  ; loss= 0.07625\n",
      "Iter 6580: acc = 0.51294, nmi = 0.00001, ari = -0.00021  ; loss= 0.02225\n",
      "Iter 6720: acc = 0.51264, nmi = 0.00001, ari = -0.00025  ; loss= 0.01201\n",
      "Iter 6860: acc = 0.50997, nmi = 0.00002, ari = -0.00025  ; loss= 0.01835\n",
      "Iter 7000: acc = 0.50647, nmi = 0.00003, ari = -0.00021  ; loss= 0.01751\n",
      "Iter 7140: acc = 0.50769, nmi = 0.00000, ari = 0.00004  ; loss= 0.01289\n",
      "Iter 7280: acc = 0.50185, nmi = 0.00008, ari = -0.00021  ; loss= 0.03455\n",
      "Iter 7420: acc = 0.50162, nmi = 0.00016, ari = -0.00032  ; loss= 0.03369\n",
      "Iter 7560: acc = 0.50113, nmi = 0.00013, ari = -0.00025  ; loss= 0.01942\n",
      "Iter 7700: acc = 0.50020, nmi = 0.00013, ari = -0.00018  ; loss= 0.10724\n",
      "Iter 7840: acc = 0.50245, nmi = 0.00025, ari = -0.00017  ; loss= 0.01716\n",
      "Iter 7980: acc = 0.50258, nmi = 0.00030, ari = -0.00020  ; loss= 0.0134\n",
      "Iter 8120: acc = 0.50452, nmi = 0.00027, ari = -0.00003  ; loss= 0.02172\n",
      "Iter 8260: acc = 0.50590, nmi = 0.00025, ari = 0.00008  ; loss= 0.01554\n",
      "Iter 8400: acc = 0.50739, nmi = 0.00038, ari = 0.00014  ; loss= 0.01289\n",
      "Iter 8540: acc = 0.51067, nmi = 0.00048, ari = 0.00042  ; loss= 0.01845\n",
      "Iter 8680: acc = 0.50655, nmi = 0.00035, ari = 0.00008  ; loss= 0.01707\n",
      "Iter 8820: acc = 0.51112, nmi = 0.00058, ari = 0.00045  ; loss= 0.04003\n",
      "Iter 8960: acc = 0.51174, nmi = 0.00066, ari = 0.00050  ; loss= 0.03398\n",
      "Iter 9100: acc = 0.51192, nmi = 0.00059, ari = 0.00053  ; loss= 0.02659\n",
      "Iter 9240: acc = 0.50752, nmi = 0.00054, ari = 0.00007  ; loss= 0.01655\n",
      "saving model to: results/ward_link_agglomerative_clustering/DEC_model_9370.h5\n",
      "Iter 9380: acc = 0.50672, nmi = 0.00063, ari = -0.00008  ; loss= 0.03483\n",
      "Iter 9520: acc = 0.51859, nmi = 0.00146, ari = 0.00132  ; loss= 0.03701\n",
      "Iter 9660: acc = 0.51976, nmi = 0.00156, ari = 0.00151  ; loss= 0.01623\n",
      "Iter 9800: acc = 0.51854, nmi = 0.00171, ari = 0.00124  ; loss= 0.01228\n",
      "Iter 9940: acc = 0.51166, nmi = 0.00080, ari = 0.00044  ; loss= 0.08373\n",
      "Iter 10080: acc = 0.50807, nmi = 0.00042, ari = 0.00019  ; loss= 0.02868\n",
      "Iter 10220: acc = 0.50637, nmi = 0.00032, ari = 0.00008  ; loss= 0.04388\n",
      "Iter 10360: acc = 0.50659, nmi = 0.00042, ari = 0.00005  ; loss= 0.01674\n",
      "Iter 10500: acc = 0.50727, nmi = 0.00048, ari = 0.00008  ; loss= 0.0092\n",
      "Iter 10640: acc = 0.50840, nmi = 0.00058, ari = 0.00014  ; loss= 0.02857\n",
      "Iter 10780: acc = 0.50946, nmi = 0.00062, ari = 0.00024  ; loss= 0.01297\n",
      "Iter 10920: acc = 0.50590, nmi = 0.00035, ari = 0.00003  ; loss= 0.0164\n",
      "Iter 11060: acc = 0.50905, nmi = 0.00040, ari = 0.00029  ; loss= 0.01267\n",
      "Iter 11200: acc = 0.51206, nmi = 0.00051, ari = 0.00056  ; loss= 0.05896\n",
      "Iter 11340: acc = 0.51207, nmi = 0.00055, ari = 0.00056  ; loss= 0.01494\n",
      "Iter 11480: acc = 0.51204, nmi = 0.00031, ari = 0.00055  ; loss= 0.08525\n",
      "Iter 11620: acc = 0.50900, nmi = 0.00014, ari = 0.00029  ; loss= 0.01588\n",
      "Iter 11760: acc = 0.50982, nmi = 0.00011, ari = 0.00032  ; loss= 0.01379\n",
      "Iter 11900: acc = 0.51059, nmi = 0.00022, ari = 0.00042  ; loss= 0.01141\n",
      "Iter 12040: acc = 0.51189, nmi = 0.00034, ari = 0.00054  ; loss= 0.0143\n",
      "Iter 12180: acc = 0.51184, nmi = 0.00021, ari = 0.00050  ; loss= 0.01457\n",
      "Iter 12320: acc = 0.51001, nmi = 0.00033, ari = 0.00038  ; loss= 0.01074\n",
      "Iter 12460: acc = 0.51164, nmi = 0.00045, ari = 0.00052  ; loss= 0.02553\n",
      "Iter 12600: acc = 0.50984, nmi = 0.00029, ari = 0.00037  ; loss= 0.0153\n",
      "Iter 12740: acc = 0.50894, nmi = 0.00037, ari = 0.00028  ; loss= 0.01369\n",
      "Iter 12880: acc = 0.50510, nmi = 0.00008, ari = 0.00009  ; loss= 0.04112\n",
      "Iter 13020: acc = 0.50649, nmi = 0.00012, ari = 0.00015  ; loss= 0.00922\n",
      "Iter 13160: acc = 0.50564, nmi = 0.00006, ari = 0.00010  ; loss= 0.01262\n",
      "Iter 13300: acc = 0.50622, nmi = 0.00005, ari = 0.00012  ; loss= 0.0101\n",
      "Iter 13440: acc = 0.51142, nmi = 0.00010, ari = 0.00038  ; loss= 0.00902\n",
      "Iter 13580: acc = 0.51416, nmi = 0.00024, ari = 0.00068  ; loss= 0.04576\n",
      "Iter 13720: acc = 0.51424, nmi = 0.00021, ari = 0.00066  ; loss= 0.00884\n",
      "Iter 13860: acc = 0.51337, nmi = 0.00018, ari = 0.00057  ; loss= 0.01784\n",
      "Iter 14000: acc = 0.51542, nmi = 0.00025, ari = 0.00078  ; loss= 0.01232\n",
      "Iter 14140: acc = 0.52019, nmi = 0.00044, ari = 0.00135  ; loss= 0.01222\n",
      "Iter 14280: acc = 0.52083, nmi = 0.00047, ari = 0.00144  ; loss= 0.02571\n",
      "Iter 14420: acc = 0.51834, nmi = 0.00025, ari = 0.00099  ; loss= 0.01752\n",
      "Iter 14560: acc = 0.51913, nmi = 0.00028, ari = 0.00108  ; loss= 0.02849\n",
      "Iter 14700: acc = 0.52024, nmi = 0.00038, ari = 0.00129  ; loss= 0.01211\n",
      "Iter 14840: acc = 0.52201, nmi = 0.00058, ari = 0.00166  ; loss= 0.00974\n",
      "Iter 14980: acc = 0.52358, nmi = 0.00072, ari = 0.00194  ; loss= 0.00959\n",
      "Iter 15120: acc = 0.52133, nmi = 0.00054, ari = 0.00155  ; loss= 0.00877\n",
      "Iter 15260: acc = 0.51836, nmi = 0.00064, ari = 0.00128  ; loss= 0.02785\n",
      "Iter 15400: acc = 0.51824, nmi = 0.00064, ari = 0.00126  ; loss= 0.00977\n",
      "Iter 15540: acc = 0.51831, nmi = 0.00059, ari = 0.00125  ; loss= 0.00754\n",
      "Iter 15680: acc = 0.51639, nmi = 0.00049, ari = 0.00101  ; loss= 0.00828\n",
      "Iter 15820: acc = 0.51578, nmi = 0.00042, ari = 0.00092  ; loss= 0.00789\n",
      "Iter 15960: acc = 0.51939, nmi = 0.00075, ari = 0.00144  ; loss= 0.02169\n",
      "Iter 16100: acc = 0.51976, nmi = 0.00079, ari = 0.00150  ; loss= 0.04168\n",
      "Iter 16240: acc = 0.52053, nmi = 0.00071, ari = 0.00157  ; loss= 0.00776\n",
      "Iter 16380: acc = 0.51859, nmi = 0.00057, ari = 0.00128  ; loss= 0.02708\n",
      "Iter 16520: acc = 0.51788, nmi = 0.00058, ari = 0.00120  ; loss= 0.00761\n",
      "Iter 16660: acc = 0.51731, nmi = 0.00062, ari = 0.00115  ; loss= 0.00988\n",
      "Iter 16800: acc = 0.51676, nmi = 0.00056, ari = 0.00107  ; loss= 0.01202\n",
      "Iter 16940: acc = 0.51576, nmi = 0.00069, ari = 0.00098  ; loss= 0.00691\n",
      "Iter 17080: acc = 0.51417, nmi = 0.00057, ari = 0.00079  ; loss= 0.01164\n",
      "Iter 17220: acc = 0.51619, nmi = 0.00070, ari = 0.00103  ; loss= 0.0291\n",
      "Iter 17360: acc = 0.51754, nmi = 0.00104, ari = 0.00121  ; loss= 0.00716\n",
      "Iter 17500: acc = 0.51901, nmi = 0.00119, ari = 0.00143  ; loss= 0.02835\n",
      "Iter 17640: acc = 0.51941, nmi = 0.00124, ari = 0.00149  ; loss= 0.04634\n",
      "Iter 17780: acc = 0.51821, nmi = 0.00098, ari = 0.00131  ; loss= 0.00846\n",
      "Iter 17920: acc = 0.51938, nmi = 0.00117, ari = 0.00148  ; loss= 0.01496\n",
      "Iter 18060: acc = 0.51893, nmi = 0.00107, ari = 0.00142  ; loss= 0.01135\n",
      "Iter 18200: acc = 0.51876, nmi = 0.00097, ari = 0.00139  ; loss= 0.01935\n",
      "Iter 18340: acc = 0.52066, nmi = 0.00127, ari = 0.00169  ; loss= 0.01357\n",
      "Iter 18480: acc = 0.52026, nmi = 0.00139, ari = 0.00162  ; loss= 0.00738\n",
      "Iter 18620: acc = 0.52026, nmi = 0.00156, ari = 0.00160  ; loss= 0.02617\n",
      "saving model to: results/ward_link_agglomerative_clustering/DEC_model_18740.h5\n",
      "Iter 18760: acc = 0.51823, nmi = 0.00132, ari = 0.00128  ; loss= 0.0203\n",
      "Iter 18900: acc = 0.51854, nmi = 0.00140, ari = 0.00132  ; loss= 0.01361\n",
      "Iter 19040: acc = 0.51746, nmi = 0.00134, ari = 0.00115  ; loss= 0.00974\n",
      "Iter 19180: acc = 0.51327, nmi = 0.00095, ari = 0.00060  ; loss= 0.01589\n",
      "Iter 19320: acc = 0.51234, nmi = 0.00086, ari = 0.00051  ; loss= 0.01434\n",
      "Iter 19460: acc = 0.51064, nmi = 0.00085, ari = 0.00028  ; loss= 0.02417\n",
      "Iter 19600: acc = 0.50939, nmi = 0.00071, ari = 0.00019  ; loss= 0.00958\n",
      "Iter 19740: acc = 0.50602, nmi = 0.00082, ari = -0.00030  ; loss= 0.04715\n",
      "Iter 19880: acc = 0.50442, nmi = 0.00064, ari = -0.00035  ; loss= 0.00695\n",
      "saving model to: results/ward_link_agglomerative_clustering/DEC_model_final.h5\n",
      "acc: 0.5044190234791889\n",
      "CPU times: user 13min 19s, sys: 1min 44s, total: 15min 3s\n",
      "Wall time: 11min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dec_model.compile(optimizer=SGD(0.01, 0.9), loss='kld')\n",
    "y_pred = dec_model.fit(train_features, y=train_labels, batch_size=32, save_dir=results_save_dir, \n",
    "                       cluster_linkage='complete')\n",
    "print('acc:', metrics.acc(train_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the features extracted by the DEC model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Output Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dec_encoder_model_pred = dec_model.encoder.predict(train_features)\n",
    "pca_mod = PCA(2)\n",
    "pca_mod.fit(dec_encoder_model_pred)\n",
    "reduced_features = pca_mod.transform(dec_encoder_model_pred)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.scatterplot(reduced_features[:, 0], reduced_features[:, 1], \n",
    "                hue=np.where(train_labels==0, 'elliptical', 'spiral'), ax=ax)\n",
    "ax.text(0.02, 0.92, f'Explained Var: {np.round(np.sum(pca_mod.explained_variance_ratio_), decimals=4)}', \n",
    "        transform=ax.transAxes)\n",
    "ax.set_xlabel('PCA Dim 1')\n",
    "ax.set_ylabel('PCA Dim 2')\n",
    "ax.set_title('PCA Scatterplot for Encoder Features')\n",
    "ax.legend(loc=4)\n",
    "plt.savefig(f'{results_save_dir}/PCAencoding_pca_features.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_log, ptrain_log = pd.read_csv(f'{results_save_dir}/dec_log.csv'), pd.read_csv(f'{results_save_dir}/pretrain_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7105afd08158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mptrain_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Reconstruction Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdec_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Clustering Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdec_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nmi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Clustering NMI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{results_save_dir}/training_curves.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "ptrain_log.plot(x='epoch', y='loss', ax=ax[0], title='Reconstruction Loss')\n",
    "dec_log.plot(x='iter', y='acc', ax=ax[1], title='Clustering Accuracy')\n",
    "dec_log.plot(x='iter', y='nmi', ax=ax[-1], title='Clustering NMI')\n",
    "plt.savefig(f'{results_save_dir}/training_curves.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
